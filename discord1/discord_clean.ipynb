{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pprint\n",
    "import emoji\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from pickle import load\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "print( tf.VERSION )\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ' # space is included in whitelist\n",
    "BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
    "FILENAME = 'tousend_mes.txt'\n",
    "limit = {\n",
    "        'maxq' : 20,\n",
    "        'minq' : 1,\n",
    "        'maxa' : 22,\n",
    "        'mina' : 1\n",
    "        }\n",
    "\n",
    "UNK = 'unk'\n",
    "# VOCAB_SIZE = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " read lines from file\n",
    "     return [list of lines]\n",
    "'''\n",
    "def read_lines(filename):\n",
    "    return open(filename, encoding='UTF-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = read_lines(FILENAME)\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функции для чистки\n",
    "'''\n",
    " split sentences in one line\n",
    "  into multiple lines\n",
    "    return [list of lines]\n",
    "'''\n",
    "def split_line_ANS(line):\n",
    "    split_line = re.split('\\.|\\!|\\?', line)\n",
    "    for el in split_line:\n",
    "        if re.search('[0-9a-zA-Z]', el):\n",
    "            return el # только первую содержательную часть ответа возвращает, до знака препинания\n",
    "    else:\n",
    "        return 'EMPTY'\n",
    "    \n",
    "def split_line_QUES(line):\n",
    "    split_line = re.split('\\.|\\!|\\?', line)\n",
    "    for el in split_line[-1::-1]:\n",
    "        if re.search('[0-9a-zA-Z]', el):\n",
    "            return el # только последнюю содержательную часть ответа возвращает, до знака препинания\n",
    "    else:\n",
    "        return 'EMPTY'    \n",
    "\n",
    "def clean_line(lines, whitelist):\n",
    "    \n",
    "    clean_lines = []\n",
    "    for i in range(0, len(lines)):\n",
    "        line = lines[i]\n",
    "        clean_line = (''.join(i for i in line if i not in emoji.UNICODE_EMOJI ))\n",
    "        clean_line = re.sub('<@\\d+>', '', clean_line)\n",
    "        clean_line = re.sub('<:.+>', '', clean_line)\n",
    "        if i % 2 == 0:\n",
    "            clean_line = split_line_QUES(clean_line)\n",
    "        else:\n",
    "            clean_line = split_line_ANS(clean_line)\n",
    "            \n",
    "\n",
    "        \n",
    "        clean_line = ''.join([ ch for ch in clean_line if ch in whitelist ])\n",
    "        clean_line = clean_line.lower()\n",
    "        clean_lines.append(clean_line)\n",
    "    return clean_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = clean_line(lines, WHITELIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_mes.txt\", 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_q, filtered_a = [], []\n",
    "for i in range(0, len(lines), 2): # перешагиваем через 1\n",
    "    qlen, alen = len(lines[i].split(' ')), len(lines[i+1].split(' '))\n",
    "    if qlen >= limit['minq'] and qlen <= limit['maxq']: # если и вопрос и ответ укладываются в длину\n",
    "        if alen >= limit['mina'] and alen <= 20:\n",
    "            filtered_q.append(lines[i]) #то  добавляем их в датасет. Если нет, то просто игнорим.\n",
    "            filtered_a.append(lines[i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 487)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_q), len(filtered_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"QUESTIONS_mes.txt\", 'w') as f:\n",
    "    for line in filtered_q:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ANSWERS_mes.txt\", 'w') as f:\n",
    "    for line in filtered_a:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_a_TAGS = list()\n",
    "for line in filtered_a:\n",
    "    filtered_a_TAGS.append( '<START> ' + line + ' <END>' )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts( filtered_q+filtered_a_TAGS ) \n",
    "tokenized_questions = tokenizer.texts_to_sequences( filtered_q ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input data shape -> (487, 20)\n"
     ]
    }
   ],
   "source": [
    "padded_questions = pad_sequences( tokenized_questions , maxlen=limit['maxq'] , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens = 1113\n"
     ]
    }
   ],
   "source": [
    "word_dict = tokenizer.word_index\n",
    "num_tokens = len( word_dict )+1\n",
    "print( 'Number of tokens = {}'.format( num_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input data shape -> (487, 22)\n"
     ]
    }
   ],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences( filtered_a_TAGS ) \n",
    "padded_answers = pad_sequences( tokenized_answers , maxlen=limit['maxa'], padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(padded_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 22)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = list()\n",
    "for token_seq in tokenized_answers:\n",
    "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
    "    \n",
    "# padded_answers = pad_sequences( decoder_target_data , maxlen=limit['maxa'], padding='post' )\n",
    "# onehot_answers = utils.to_categorical( padded_answers , num_tokens )\n",
    "# decoder_target_data = np.array( onehot_answers )\n",
    "# print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23, 18, 49, 876, 7, 10, 877, 8, 878, 2],\n",
       " [13, 36, 12, 43, 370, 76, 2],\n",
       " [75, 21, 2],\n",
       " [75, 2]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_answers = pad_sequences( decoder_target_data , maxlen=limit['maxa'], padding='post' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 23,  18,  49, ...,   0,   0,   0],\n",
       "       [ 13,  36,  12, ...,   0,   0,   0],\n",
       "       [ 75,  21,   2, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  3, 347, 162, ...,   0,   0,   0],\n",
       "       [ 29,   2,   0, ...,   0,   0,   0],\n",
       "       [  3, 270,  24, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487\n",
      "(487, 22)\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_answers))\n",
    "print(padded_answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(padded_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144,\n",
       "       145, 147, 148, 151, 152, 153, 154, 155, 156, 158, 159, 160, 162,\n",
       "       163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
       "       177, 178, 179, 183, 185, 186, 187, 188, 189, 190, 191, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208,\n",
       "       209, 213, 214, 215, 216])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(np.unique(padded_answers)))\n",
    "np.unique(padded_answers)[0:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11924682"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22*487*1113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_answers = utils.to_categorical( padded_answers , num_tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(487, 22, 1113)\n"
     ]
    }
   ],
   "source": [
    "print(type(onehot_answers))\n",
    "print(onehot_answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "1113\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(len(onehot_answers[0]))\n",
    "print((onehot_answers[0]))\n",
    "print(len(onehot_answers[0][0]))\n",
    "print((onehot_answers[0][0]))\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lera\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    222600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    222600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1113)   223713      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,310,513\n",
      "Trainable params: 1,310,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True ) (encoder_inputs)\n",
    "\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-CHAT-discord_improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.9604\n",
      "Epoch 00001: loss improved from 1.00936 to 0.96458, saving model to weights-CHAT-discord_improvement-01-0.9646.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.9646\n",
      "Epoch 2/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.9196\n",
      "Epoch 00002: loss improved from 0.96458 to 0.91790, saving model to weights-CHAT-discord_improvement-02-0.9179.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.9179\n",
      "Epoch 3/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.8889\n",
      "Epoch 00003: loss improved from 0.91790 to 0.88396, saving model to weights-CHAT-discord_improvement-03-0.8840.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.8840\n",
      "Epoch 4/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.8295\n",
      "Epoch 00004: loss improved from 0.88396 to 0.84436, saving model to weights-CHAT-discord_improvement-04-0.8444.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.8444\n",
      "Epoch 5/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.8377\n",
      "Epoch 00005: loss improved from 0.84436 to 0.83320, saving model to weights-CHAT-discord_improvement-05-0.8332.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.8332\n",
      "Epoch 6/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.7801\n",
      "Epoch 00006: loss improved from 0.83320 to 0.78444, saving model to weights-CHAT-discord_improvement-06-0.7844.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.7844\n",
      "Epoch 7/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.7525\n",
      "Epoch 00007: loss improved from 0.78444 to 0.75277, saving model to weights-CHAT-discord_improvement-07-0.7528.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.7528\n",
      "Epoch 8/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.7340\n",
      "Epoch 00008: loss improved from 0.75277 to 0.73050, saving model to weights-CHAT-discord_improvement-08-0.7305.hdf5\n",
      "487/487 [==============================] - 3s 7ms/sample - loss: 0.7305\n",
      "Epoch 9/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.6823\n",
      "Epoch 00009: loss improved from 0.73050 to 0.68841, saving model to weights-CHAT-discord_improvement-09-0.6884.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.6884\n",
      "Epoch 10/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.6505\n",
      "Epoch 00010: loss improved from 0.68841 to 0.65476, saving model to weights-CHAT-discord_improvement-10-0.6548.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.6548\n",
      "Epoch 11/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.6390\n",
      "Epoch 00011: loss improved from 0.65476 to 0.64072, saving model to weights-CHAT-discord_improvement-11-0.6407.hdf5\n",
      "487/487 [==============================] - 3s 7ms/sample - loss: 0.6407\n",
      "Epoch 12/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.6001\n",
      "Epoch 00012: loss improved from 0.64072 to 0.60115, saving model to weights-CHAT-discord_improvement-12-0.6012.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.6012\n",
      "Epoch 13/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.5744\n",
      "Epoch 00013: loss improved from 0.60115 to 0.57564, saving model to weights-CHAT-discord_improvement-13-0.5756.hdf5\n",
      "487/487 [==============================] - 3s 7ms/sample - loss: 0.5756\n",
      "Epoch 14/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.5393\n",
      "Epoch 00014: loss improved from 0.57564 to 0.54863, saving model to weights-CHAT-discord_improvement-14-0.5486.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.5486\n",
      "Epoch 15/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.5227\n",
      "Epoch 00015: loss improved from 0.54863 to 0.52426, saving model to weights-CHAT-discord_improvement-15-0.5243.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.5243\n",
      "Epoch 16/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.4945\n",
      "Epoch 00016: loss improved from 0.52426 to 0.50182, saving model to weights-CHAT-discord_improvement-16-0.5018.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.5018\n",
      "Epoch 17/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.4788\n",
      "Epoch 00017: loss improved from 0.50182 to 0.48206, saving model to weights-CHAT-discord_improvement-17-0.4821.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.4821\n",
      "Epoch 18/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.4612\n",
      "Epoch 00018: loss improved from 0.48206 to 0.46243, saving model to weights-CHAT-discord_improvement-18-0.4624.hdf5\n",
      "487/487 [==============================] - 3s 7ms/sample - loss: 0.4624\n",
      "Epoch 19/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.4338\n",
      "Epoch 00019: loss improved from 0.46243 to 0.43494, saving model to weights-CHAT-discord_improvement-19-0.4349.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.4349\n",
      "Epoch 20/20\n",
      "450/487 [==========================>...] - ETA: 0s - loss: 0.4211\n",
      "Epoch 00020: loss improved from 0.43494 to 0.42013, saving model to weights-CHAT-discord_improvement-20-0.4201.hdf5\n",
      "487/487 [==============================] - 3s 6ms/sample - loss: 0.4201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20d4a7cb550>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=20, callbacks=callbacks_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len = encoder_input_data.shape[1]\n",
    "max_answer_len = decoder_input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokens_list.append( word_dict[ word ] ) \n",
    "        except:\n",
    "            continue\n",
    "    return pad_sequences( [tokens_list] , maxlen=max_question_len , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : how are you\n",
      " im good how about you end\n",
      "Enter question : whats up\n",
      " hajime hinata tiki from fe chiaki nanami and joshua from fe end\n",
      "Enter question : up\n",
      " wassup end\n",
      "Enter question : whats\n",
      " wassup end\n",
      "Enter question : whats up\n",
      " hajime hinata tiki from fe chiaki nanami and joshua from fe end\n",
      "Enter question : how about mango\n",
      " nice end\n",
      "Enter question : nice? he is evil\n",
      " hajime hinata tiki from fe chiaki nanami and joshua from fe end\n",
      "Enter question : olala olala be\n",
      " hahaha end\n",
      "Enter question : you are a nice chatbot\n",
      " nice i forgot how haha end\n",
      "Enter question : very informative\n",
      " wassup end\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10): # 10 вопросов можно\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )#1.   First, we take a question as input and predict the state values using `enc_model`.\n",
    "\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )#2.   We set the state values in the decoder's LSTM.\n",
    "\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_answer_len:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
