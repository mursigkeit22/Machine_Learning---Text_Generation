{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1714,
     "status": "ok",
     "timestamp": 1556943354017,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "w2lAX668DPCP",
    "outputId": "93f7145c-3eb8-4757-9846-343fd508963f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto(allow_soft_placement=True)\n",
    "# config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем, какая видюха, результаты в черном окошке с юпитером\n",
    "## Creates a graph.\n",
    "# a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "# b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "# c = tf.matmul(a, b)\n",
    "# # Creates a session with log_device_placement set to True.\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# # Runs the op.\n",
    "# print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgrNBqFlFIxC"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r',encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmqViCrNIa4q"
   },
   "outputs": [],
   "source": [
    "in_filename = 'gk4z_df_10-20_TXT.txt'\n",
    "data = load_doc(in_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeVhgE6sIz9W"
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "tokenizer = load(open('tokenizer_gen_7_TOKENS10-20PADDING.pkl',  'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOanmpgFEzqJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "corpus = data.split(\"\\n\")    \n",
    "# tokenizer.fit_on_texts(corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1556945031515,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "rFLj-RlrEzj7",
    "outputId": "63eb2a0a-efa6-4330-b3f1-7d535977a697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  19794\n",
      "number of lines:  7206\n",
      "all the words:  101689\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size: ', vocab_size)\n",
    "print('number of lines: ', len(corpus))\n",
    "print('all the words: ', len(data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8trWAgeCEZCJ"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "  for i in range (1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1556945924495,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "uaALyjiPNAn_",
    "outputId": "4feebe67-091a-4859-c949-3022bfd71afb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sL4E6A4PNAkf"
   },
   "outputs": [],
   "source": [
    "input_sequences = array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1556947827334,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "jbQ987cnUhSM",
    "outputId": "b3b7c004-961a-4b5f-df49-5a5950647760"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94483, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,  1494,  7043],\n",
       "       [    0,     0,     0, ...,  1494,  7043,  7044],\n",
       "       [    0,     0,     0, ...,  7043,  7044,    47],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  1287,   719,    34],\n",
       "       [    0,     0,     0, ...,   719,    34,     1],\n",
       "       [    0,     0,     0, ...,    34,     1, 19793]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjyjr0YwNAes"
   },
   "outputs": [],
   "source": [
    "\n",
    "X,y = input_sequences[:,:-1],input_sequences[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5txRjZJOAIm"
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94483, 19794)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1556946404577,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "IXpVC4UpEYgE",
    "outputId": "1200992a-120a-48d3-d8ef-b0ec6084a388"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model__gen_7_TOKENS10-20PADDING.h5')\n",
    "model.load_weights('weights-TOK10-20PAD_gen7_improv-38-2.8597.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fg-WFOudTCBS"
   },
   "source": [
    "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
    "\n",
    "Common values are 50, 100, and 300. We will use 50 here, but consider testing smaller or larger values.\n",
    "\n",
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
    "\n",
    "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1556947515745,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "s2tuaheaPRWY",
    "outputId": "b8987a0f-6f70-409e-ce05-4ed3ac81dd2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 300)           5938200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 18, 300)           721200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 18, 300)           721200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19794)             5957994   \n",
      "=================================================================\n",
      "Total params: 14,059,794\n",
      "Trainable params: 14,059,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "# model.add(layers.Embedding(vocab_size, 300, input_length=seq_length))\n",
    "# model.add(LSTM(300, return_sequences=True))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(LSTM(300, return_sequences=True))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(LSTM(300))\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epVu2SVnPRFl"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4GM8tdXO9U5"
   },
   "outputs": [],
   "source": [
    "filepath=\"weights-TOK10-20PAD_gen7_improv-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1,  min_lr=0.000001, verbose=1,cooldown=0)\n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 119789,
     "status": "error",
     "timestamp": 1556947807641,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "ZXTgvIUdO9Pn",
    "outputId": "9d6b31f1-afde-4252-8572-c4deae361dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "94483/94483 [==============================] - 369s 4ms/step - loss: 3.1258\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.12577, saving model to weights-TOK10-20PAD_gen7_improv-01-3.1258.hdf5\n",
      "Epoch 2/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 3.1697\n",
      "\n",
      "Epoch 00002: loss did not improve from 3.12577\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 3/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 3.0333\n",
      "\n",
      "Epoch 00003: loss improved from 3.12577 to 3.03334, saving model to weights-TOK10-20PAD_gen7_improv-03-3.0333.hdf5\n",
      "Epoch 4/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.9764\n",
      "\n",
      "Epoch 00004: loss improved from 3.03334 to 2.97644, saving model to weights-TOK10-20PAD_gen7_improv-04-2.9764.hdf5\n",
      "Epoch 5/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.9565\n",
      "\n",
      "Epoch 00005: loss improved from 2.97644 to 2.95652, saving model to weights-TOK10-20PAD_gen7_improv-05-2.9565.hdf5\n",
      "Epoch 6/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.9385\n",
      "\n",
      "Epoch 00006: loss improved from 2.95652 to 2.93851, saving model to weights-TOK10-20PAD_gen7_improv-06-2.9385.hdf5\n",
      "Epoch 7/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.9270\n",
      "\n",
      "Epoch 00007: loss improved from 2.93851 to 2.92703, saving model to weights-TOK10-20PAD_gen7_improv-07-2.9270.hdf5\n",
      "Epoch 8/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.9197\n",
      "\n",
      "Epoch 00008: loss improved from 2.92703 to 2.91970, saving model to weights-TOK10-20PAD_gen7_improv-08-2.9197.hdf5\n",
      "Epoch 9/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.9153\n",
      "\n",
      "Epoch 00009: loss improved from 2.91970 to 2.91529, saving model to weights-TOK10-20PAD_gen7_improv-09-2.9153.hdf5\n",
      "Epoch 10/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.9094\n",
      "\n",
      "Epoch 00010: loss improved from 2.91529 to 2.90942, saving model to weights-TOK10-20PAD_gen7_improv-10-2.9094.hdf5\n",
      "Epoch 11/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.9015\n",
      "\n",
      "Epoch 00011: loss improved from 2.90942 to 2.90147, saving model to weights-TOK10-20PAD_gen7_improv-11-2.9015.hdf5\n",
      "Epoch 12/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8984\n",
      "\n",
      "Epoch 00012: loss improved from 2.90147 to 2.89838, saving model to weights-TOK10-20PAD_gen7_improv-12-2.8984.hdf5\n",
      "Epoch 13/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8958\n",
      "\n",
      "Epoch 00013: loss improved from 2.89838 to 2.89579, saving model to weights-TOK10-20PAD_gen7_improv-13-2.8958.hdf5\n",
      "Epoch 14/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8951\n",
      "\n",
      "Epoch 00014: loss improved from 2.89579 to 2.89513, saving model to weights-TOK10-20PAD_gen7_improv-14-2.8951.hdf5\n",
      "Epoch 15/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8857\n",
      "\n",
      "Epoch 00015: loss improved from 2.89513 to 2.88570, saving model to weights-TOK10-20PAD_gen7_improv-15-2.8857.hdf5\n",
      "Epoch 16/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8877\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.88570\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 17/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8675\n",
      "\n",
      "Epoch 00017: loss improved from 2.88570 to 2.86754, saving model to weights-TOK10-20PAD_gen7_improv-17-2.8675.hdf5\n",
      "Epoch 18/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8639\n",
      "\n",
      "Epoch 00018: loss improved from 2.86754 to 2.86392, saving model to weights-TOK10-20PAD_gen7_improv-18-2.8639.hdf5\n",
      "Epoch 19/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8590\n",
      "\n",
      "Epoch 00019: loss improved from 2.86392 to 2.85905, saving model to weights-TOK10-20PAD_gen7_improv-19-2.8590.hdf5\n",
      "Epoch 20/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8580\n",
      "\n",
      "Epoch 00020: loss improved from 2.85905 to 2.85801, saving model to weights-TOK10-20PAD_gen7_improv-20-2.8580.hdf5\n",
      "Epoch 21/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8567\n",
      "\n",
      "Epoch 00021: loss improved from 2.85801 to 2.85670, saving model to weights-TOK10-20PAD_gen7_improv-21-2.8567.hdf5\n",
      "Epoch 22/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8554\n",
      "\n",
      "Epoch 00022: loss improved from 2.85670 to 2.85544, saving model to weights-TOK10-20PAD_gen7_improv-22-2.8554.hdf5\n",
      "Epoch 23/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8542\n",
      "\n",
      "Epoch 00023: loss improved from 2.85544 to 2.85416, saving model to weights-TOK10-20PAD_gen7_improv-23-2.8542.hdf5\n",
      "Epoch 24/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8523\n",
      "\n",
      "Epoch 00024: loss improved from 2.85416 to 2.85230, saving model to weights-TOK10-20PAD_gen7_improv-24-2.8523.hdf5\n",
      "Epoch 25/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8506\n",
      "\n",
      "Epoch 00025: loss improved from 2.85230 to 2.85058, saving model to weights-TOK10-20PAD_gen7_improv-25-2.8506.hdf5\n",
      "Epoch 26/100\n",
      "94483/94483 [==============================] - 365s 4ms/step - loss: 2.8524\n",
      "\n",
      "Epoch 00026: loss did not improve from 2.85058\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 27/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8478\n",
      "\n",
      "Epoch 00027: loss improved from 2.85058 to 2.84785, saving model to weights-TOK10-20PAD_gen7_improv-27-2.8478.hdf5\n",
      "Epoch 28/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8481\n",
      "\n",
      "Epoch 00028: loss did not improve from 2.84785\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 29/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8471\n",
      "\n",
      "Epoch 00029: loss improved from 2.84785 to 2.84714, saving model to weights-TOK10-20PAD_gen7_improv-29-2.8471.hdf5\n",
      "Epoch 30/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8452\n",
      "\n",
      "Epoch 00030: loss improved from 2.84714 to 2.84516, saving model to weights-TOK10-20PAD_gen7_improv-30-2.8452.hdf5\n",
      "Epoch 31/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8457\n",
      "\n",
      "Epoch 00031: loss did not improve from 2.84516\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 32/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8432\n",
      "\n",
      "Epoch 00032: loss improved from 2.84516 to 2.84325, saving model to weights-TOK10-20PAD_gen7_improv-32-2.8432.hdf5\n",
      "Epoch 33/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8441\n",
      "\n",
      "Epoch 00033: loss did not improve from 2.84325\n",
      "Epoch 34/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8453\n",
      "\n",
      "Epoch 00034: loss did not improve from 2.84325\n",
      "Epoch 35/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8440\n",
      "\n",
      "Epoch 00035: loss did not improve from 2.84325\n",
      "Epoch 36/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8467\n",
      "\n",
      "Epoch 00036: loss did not improve from 2.84325\n",
      "Epoch 37/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8457\n",
      "\n",
      "Epoch 00037: loss did not improve from 2.84325\n",
      "Epoch 38/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8445\n",
      "\n",
      "Epoch 00038: loss did not improve from 2.84325\n",
      "Epoch 39/100\n",
      "94483/94483 [==============================] - 365s 4ms/step - loss: 2.8449\n",
      "\n",
      "Epoch 00039: loss did not improve from 2.84325\n",
      "Epoch 40/100\n",
      "94483/94483 [==============================] - 367s 4ms/step - loss: 2.8448\n",
      "\n",
      "Epoch 00040: loss did not improve from 2.84325\n",
      "Epoch 41/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8444\n",
      "\n",
      "Epoch 00041: loss did not improve from 2.84325\n",
      "Epoch 42/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8443\n",
      "\n",
      "Epoch 00042: loss did not improve from 2.84325\n",
      "Epoch 43/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8436\n",
      "\n",
      "Epoch 00043: loss did not improve from 2.84325\n",
      "Epoch 44/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: loss did not improve from 2.84325\n",
      "Epoch 45/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8447\n",
      "\n",
      "Epoch 00045: loss did not improve from 2.84325\n",
      "Epoch 46/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8444\n",
      "\n",
      "Epoch 00046: loss did not improve from 2.84325\n",
      "Epoch 47/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8449\n",
      "\n",
      "Epoch 00047: loss did not improve from 2.84325\n",
      "Epoch 48/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8444\n",
      "\n",
      "Epoch 00048: loss did not improve from 2.84325\n",
      "Epoch 49/100\n",
      "94483/94483 [==============================] - 361s 4ms/step - loss: 2.8456\n",
      "\n",
      "Epoch 00049: loss did not improve from 2.84325\n",
      "Epoch 50/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8434\n",
      "\n",
      "Epoch 00050: loss did not improve from 2.84325\n",
      "Epoch 51/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8428\n",
      "\n",
      "Epoch 00051: loss improved from 2.84325 to 2.84284, saving model to weights-TOK10-20PAD_gen7_improv-51-2.8428.hdf5\n",
      "Epoch 52/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8437\n",
      "\n",
      "Epoch 00052: loss did not improve from 2.84284\n",
      "Epoch 53/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8442\n",
      "\n",
      "Epoch 00053: loss did not improve from 2.84284\n",
      "Epoch 54/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8433\n",
      "\n",
      "Epoch 00054: loss did not improve from 2.84284\n",
      "Epoch 55/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8428\n",
      "\n",
      "Epoch 00055: loss improved from 2.84284 to 2.84283, saving model to weights-TOK10-20PAD_gen7_improv-55-2.8428.hdf5\n",
      "Epoch 56/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8433\n",
      "\n",
      "Epoch 00056: loss did not improve from 2.84283\n",
      "Epoch 57/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8417\n",
      "\n",
      "Epoch 00057: loss improved from 2.84283 to 2.84170, saving model to weights-TOK10-20PAD_gen7_improv-57-2.8417.hdf5\n",
      "Epoch 58/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8449\n",
      "\n",
      "Epoch 00058: loss did not improve from 2.84170\n",
      "Epoch 59/100\n",
      "94483/94483 [==============================] - 365s 4ms/step - loss: 2.8424\n",
      "\n",
      "Epoch 00059: loss did not improve from 2.84170\n",
      "Epoch 60/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8434\n",
      "\n",
      "Epoch 00060: loss did not improve from 2.84170\n",
      "Epoch 61/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8436\n",
      "\n",
      "Epoch 00061: loss did not improve from 2.84170\n",
      "Epoch 62/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8435\n",
      "\n",
      "Epoch 00062: loss did not improve from 2.84170\n",
      "Epoch 63/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8452\n",
      "\n",
      "Epoch 00063: loss did not improve from 2.84170\n",
      "Epoch 64/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8440\n",
      "\n",
      "Epoch 00064: loss did not improve from 2.84170\n",
      "Epoch 65/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8424\n",
      "\n",
      "Epoch 00065: loss did not improve from 2.84170\n",
      "Epoch 66/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8425\n",
      "\n",
      "Epoch 00066: loss did not improve from 2.84170\n",
      "Epoch 67/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8452\n",
      "\n",
      "Epoch 00067: loss did not improve from 2.84170\n",
      "Epoch 68/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8423\n",
      "\n",
      "Epoch 00068: loss did not improve from 2.84170\n",
      "Epoch 69/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8424\n",
      "\n",
      "Epoch 00069: loss did not improve from 2.84170\n",
      "Epoch 70/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8421\n",
      "\n",
      "Epoch 00070: loss did not improve from 2.84170\n",
      "Epoch 71/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8433\n",
      "\n",
      "Epoch 00071: loss did not improve from 2.84170\n",
      "Epoch 72/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8417\n",
      "\n",
      "Epoch 00072: loss did not improve from 2.84170\n",
      "Epoch 73/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8444\n",
      "\n",
      "Epoch 00073: loss did not improve from 2.84170\n",
      "Epoch 74/100\n",
      "94483/94483 [==============================] - 365s 4ms/step - loss: 2.8438\n",
      "\n",
      "Epoch 00074: loss did not improve from 2.84170\n",
      "Epoch 75/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8421\n",
      "\n",
      "Epoch 00075: loss did not improve from 2.84170\n",
      "Epoch 76/100\n",
      "94483/94483 [==============================] - 365s 4ms/step - loss: 2.8424\n",
      "\n",
      "Epoch 00076: loss did not improve from 2.84170\n",
      "Epoch 77/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8440\n",
      "\n",
      "Epoch 00077: loss did not improve from 2.84170\n",
      "Epoch 78/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8430\n",
      "\n",
      "Epoch 00078: loss did not improve from 2.84170\n",
      "Epoch 79/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8428\n",
      "\n",
      "Epoch 00079: loss did not improve from 2.84170\n",
      "Epoch 80/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8423\n",
      "\n",
      "Epoch 00080: loss did not improve from 2.84170\n",
      "Epoch 81/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8412\n",
      "\n",
      "Epoch 00081: loss improved from 2.84170 to 2.84120, saving model to weights-TOK10-20PAD_gen7_improv-81-2.8412.hdf5\n",
      "Epoch 82/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8420\n",
      "\n",
      "Epoch 00082: loss did not improve from 2.84120\n",
      "Epoch 83/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8451\n",
      "\n",
      "Epoch 00083: loss did not improve from 2.84120\n",
      "Epoch 84/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8451\n",
      "\n",
      "Epoch 00084: loss did not improve from 2.84120\n",
      "Epoch 85/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8448\n",
      "\n",
      "Epoch 00085: loss did not improve from 2.84120\n",
      "Epoch 86/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8427\n",
      "\n",
      "Epoch 00086: loss did not improve from 2.84120\n",
      "Epoch 87/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8455\n",
      "\n",
      "Epoch 00087: loss did not improve from 2.84120\n",
      "Epoch 88/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8428\n",
      "\n",
      "Epoch 00088: loss did not improve from 2.84120\n",
      "Epoch 89/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8421\n",
      "\n",
      "Epoch 00089: loss did not improve from 2.84120\n",
      "Epoch 90/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8443\n",
      "\n",
      "Epoch 00090: loss did not improve from 2.84120\n",
      "Epoch 91/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8420\n",
      "\n",
      "Epoch 00091: loss did not improve from 2.84120\n",
      "Epoch 92/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8443\n",
      "\n",
      "Epoch 00092: loss did not improve from 2.84120\n",
      "Epoch 93/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8427\n",
      "\n",
      "Epoch 00093: loss did not improve from 2.84120\n",
      "Epoch 94/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8414\n",
      "\n",
      "Epoch 00094: loss did not improve from 2.84120\n",
      "Epoch 95/100\n",
      "94483/94483 [==============================] - 362s 4ms/step - loss: 2.8425\n",
      "\n",
      "Epoch 00095: loss did not improve from 2.84120\n",
      "Epoch 96/100\n",
      "94483/94483 [==============================] - 365s 4ms/step - loss: 2.8441\n",
      "\n",
      "Epoch 00096: loss did not improve from 2.84120\n",
      "Epoch 97/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8427\n",
      "\n",
      "Epoch 00097: loss did not improve from 2.84120\n",
      "Epoch 98/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8420\n",
      "\n",
      "Epoch 00098: loss did not improve from 2.84120\n",
      "Epoch 99/100\n",
      "94483/94483 [==============================] - 364s 4ms/step - loss: 2.8431\n",
      "\n",
      "Epoch 00099: loss did not improve from 2.84120\n",
      "Epoch 100/100\n",
      "94483/94483 [==============================] - 363s 4ms/step - loss: 2.8419\n",
      "\n",
      "Epoch 00100: loss did not improve from 2.84120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29707934cc0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbqYig4mO9Ko"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model__gen_7_TOKENS10-20PADDING.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_gen_7_TOKENS10-20PADDING.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, n_words, max_sequence_len, model):\n",
    "#     print(seed_text)\n",
    "    result = ''\n",
    "    for j in range(n_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "#         print('token_list ', token_list)\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "#         print('pad_token_list ', token_list)\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "#         print('predicted ', predicted)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                result += \" \" + output_word\n",
    "#                 print('output_word ', output_word)\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "#         print('seed_text ', seed_text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "у второй половинки чгкз находится на первом месте в списке пабликов считается ли это за идеальный выбор человека\n",
      " ночь бабла власти ебанутые эффектами умеют выход концертов президент приехал\n",
      "\n",
      "спасибо что без мьюз и аймэикс не гоню на альбомы просто достали в каждый топ их включать\n",
      " все плейлиста класс земфира стоило приделывать будет сообщений россию ночь\n",
      "\n",
      "заебись когда показываешь басисту что хочешь пить и он даёт тебе водички\n",
      " музыкант не хватает а друг тут квартетов лол лол мои\n",
      "\n",
      "чем больше концертов посещаю тем больше песен не могу слушать потому что воспоминания нормально ли это\n",
      " за сына встречаться нужно добавил библии так купить спокойно президент\n",
      "\n",
      "5 сентября пойти на аэросмит на лубянке или на макса коржа в лужниках\n",
      " клуб только пытаться делаю в инстаграм чтобы обезопасит купить обезопасит\n",
      "\n",
      "в этот день 5 лет назад весь мир узнал как сильно можно любить группу\n",
      " чтобы запись присылают не надоела разножанровые понимаю ждём концерт концерт\n",
      "\n",
      "кто выпил бухло спрятанное около арены выпуск передачи криминальная россия смотреть 720р\n",
      " ходит туда так мне любит остальными думаете круто концерт власти\n",
      "\n",
      "больше не хочется ставить любимых музыкантов на аватарку это взросление или старость\n",
      " это так как так много меня поднимать дело месяц разножанровые\n",
      "\n",
      "почему женщины в группах в основном всегда только на вокале они же не только микрофон у рта держать умеют\n",
      " зрением выход будет получается ли закрылся важнее бабла извините бухать\n",
      "\n",
      "если в новоульяновске будут рейвы в заброшенных церквях я так и быть напишу одну\n",
      " песню как перестать работает жахнуть полностью связи посередине нет любимое\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "  seed_text = corpus[randint(0,len(corpus))]\n",
    "  print(seed_text)\n",
    "  generated = generate_text(seed_text, 10,  max_sequence_len, model)\n",
    "  print(generated+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gk4z_gen_6_TOKENS10-20PADDING.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
