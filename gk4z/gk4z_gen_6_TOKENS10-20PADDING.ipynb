{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1714,
     "status": "ok",
     "timestamp": 1556943354017,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "w2lAX668DPCP",
    "outputId": "93f7145c-3eb8-4757-9846-343fd508963f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgrNBqFlFIxC"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r',encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmqViCrNIa4q"
   },
   "outputs": [],
   "source": [
    "in_filename = 'gk4z_df_10-20_TXT.txt'\n",
    "data = load_doc(in_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeVhgE6sIz9W"
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "tokenizer = load(open('tokenizer_gen_6_TOKENS10-20PADDING.pkl',  'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOanmpgFEzqJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "corpus = data.split(\"\\n\")    \n",
    "tokenizer.fit_on_texts(corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1556945031515,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "rFLj-RlrEzj7",
    "outputId": "63eb2a0a-efa6-4330-b3f1-7d535977a697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  19794\n",
      "number of lines:  7206\n",
      "all the words:  101689\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size: ', vocab_size)\n",
    "print('number of lines: ', len(corpus))\n",
    "print('all the words: ', len(data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8trWAgeCEZCJ"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "  for i in range (1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1556945924495,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "uaALyjiPNAn_",
    "outputId": "4feebe67-091a-4859-c949-3022bfd71afb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sL4E6A4PNAkf"
   },
   "outputs": [],
   "source": [
    "input_sequences = array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1556947827334,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "jbQ987cnUhSM",
    "outputId": "b3b7c004-961a-4b5f-df49-5a5950647760"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94483, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjyjr0YwNAes"
   },
   "outputs": [],
   "source": [
    "\n",
    "X,y = input_sequences[:,:-1],input_sequences[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5txRjZJOAIm"
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94483, 19794)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1556946404577,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "IXpVC4UpEYgE",
    "outputId": "1200992a-120a-48d3-d8ef-b0ec6084a388"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model__gen_6_TOKENS10-20PADDING.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fg-WFOudTCBS"
   },
   "source": [
    "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
    "\n",
    "Common values are 50, 100, and 300. We will use 50 here, but consider testing smaller or larger values.\n",
    "\n",
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
    "\n",
    "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1556947515745,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "s2tuaheaPRWY",
    "outputId": "b8987a0f-6f70-409e-ce05-4ed3ac81dd2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 50)            989700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 18, 300)           421200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19794)             5957994   \n",
      "=================================================================\n",
      "Total params: 8,090,094\n",
      "Trainable params: 8,090,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 50, input_length=seq_length, mask_zero = True))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epVu2SVnPRFl"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4GM8tdXO9U5"
   },
   "outputs": [],
   "source": [
    "filepath=\"weights-TOKENS-10-20PADDINGimprovement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1,  min_lr=0.000001, verbose=1,cooldown=1)\n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 119789,
     "status": "error",
     "timestamp": 1556947807641,
     "user": {
      "displayName": "Valerie Shelgunova",
      "photoUrl": "https://lh5.googleusercontent.com/-i2CX_QSsamo/AAAAAAAAAAI/AAAAAAAAAB8/YRSVdMujpXw/s64/photo.jpg",
      "userId": "14712883095412086889"
     },
     "user_tz": -180
    },
    "id": "ZXTgvIUdO9Pn",
    "outputId": "9d6b31f1-afde-4252-8572-c4deae361dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 1408/94483 [..............................] - ETA: 1:07:32 - loss: 9.3908"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-901f39456e34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbqYig4mO9Ko"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model__gen_6_TOKENS10-20PADDING.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_gen_6_TOKENS10-20PADDING.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights-TOKENS-10-20PADDINGimprovement-29-1.3047.hdf5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n",
      "token_list  [11509]\n",
      "pad_token_list  [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 11509]]\n",
      "predicted  [1]\n",
      "в\n"
     ]
    }
   ],
   "source": [
    "# weights-TOKENS-10-20PADDINGimprovement-11-3.8014.hdf5\n",
    "\n",
    "# weights-TOKENS-10-20PADDINGimprovement-16-2.6663.hdf5\n",
    "\n",
    "# weights-TOKENS-10-20PADDINGimprovement-21-1.9680.hdf5\n",
    "        \n",
    "# weights-TOKENS-10-20PADDINGimprovement-29-1.3047.hdf5      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, n_words, max_sequence_len, model):\n",
    "#     print(seed_text)\n",
    "    result = ''\n",
    "    for j in range(n_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "#         print('token_list ', token_list)\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "#         print('pad_token_list ', token_list)\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "#         print('predicted ', predicted)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                result += \" \" + output_word\n",
    "#                 print('output_word ', output_word)\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "#         print('seed_text ', seed_text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' их трек и еще прямо во сне уже с афтепати'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('ариана ты выиграла грэмми приходи за статуэткой ох нет она в своих фирменных наушниках она нас не слышит', 10, max_sequence_len, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "может уже эминема наконец ну и бринги давно уже были пора бы ещё раз\n",
      "пора вроде пора бы песня из сетлиста\n",
      "\n",
      "если я послушаю слитый альбом до релиза буду ли я грешником\n",
      "бы грешником был ставь быть можно ли\n",
      "\n",
      "сколько фоток и видосов с одного концерта можно выложить в инстаграм чтобы не начали отписываться\n",
      "начали на нем и случайно любят смерти\n",
      "\n",
      "30 секунд до джаред ты ебаный пенсионер кто тебе в уши насрал сука пиздец\n",
      "без собой название фестивалей не был ее\n",
      "\n",
      "долго пытался найти тёрнера где этот залаченнный франт что это за поздний леннон блин\n",
      "дайте поздний голос похож на концерты с\n",
      "\n",
      "ну он вроде бы не приезжал в россию а пост направлен на непонятный вброс без конкретного примера\n",
      "себе работает станет нет билеты на любой\n",
      "\n",
      "ну их и так выделяют как отдельные жанры чо за вопрос\n",
      "вот когда ты понравиться мутится получше в\n",
      "\n",
      "кому на бесплатном концерте заплатить за билет чтобы избежать всего этого безумия\n",
      "безумия был ставь лайк когда тоже устанут\n",
      "\n",
      "это годнота на самом деле скажу как человек такое не слушающий\n",
      "билет на чм потому что не в\n",
      "\n",
      "группа выложила в свой аккаунт мое фото с концерта когда ждать приглашения стать штатным фотографом в туре\n",
      "где попфарм не кидают в 2019 году\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "  seed_text = corpus[randint(0,len(corpus))]\n",
    "  print(seed_text)\n",
    "  generated = generate_seq(model, tokenizer, seq_length, seed_text, 7)\n",
    "  print(generated+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 еще не наступил но я уже коплю деньги на концерты я зависима или нет\n",
      " делаю нормально ли это программа сестра в фб подвох приделывать\n",
      "\n",
      "мама сказала найти символ 2017 года на застолье подойдёт ли фотка ури\n",
      " парикмахеру для меня не убирает его в чгкз уроки разножанровые\n",
      "\n",
      "жизненноменя сразу бомбит хотя здравый смысл подсказывает что у каждого свой вкус но чёт это не помогает\n",
      " станет взросление или говорит что не выпендриваться твою названий группы\n",
      "\n",
      "за неделю 3 поста опубликовали моих я просто в шоке нахожусь если честно\n",
      " слушали и манит не разрезая их баллы детей быстрее лет\n",
      "\n",
      "оливер сайкс заболел после концерта в киеве заметим ли мы разницу в вокале\n",
      " заметим ли это идиоты репетицией получается петь как бывшей шутку\n",
      "\n",
      "не нашел себя ни на одной фотке с концерта какой яд более действенный\n",
      " его написано и согласились здоровья не так сделала и бывшей\n",
      "\n",
      "знают ли девочки пытающиеся вписаться на концерты об ориентации музыкантов заранее или бывает нежданчик\n",
      " недостойной или успеха на сенсейшна форма библия мира наоборот заниматься\n",
      "\n",
      "что делать если анонсы вызывают не радость а слезы и вопрос откуда сколько денег взять\n",
      " а не замерзнуть в социальных ползти гитлер нет лица назад\n",
      "\n",
      "что лучше тренирует выносливость поход в горы или 10часовая очередь на концерт\n",
      " любимой группы в россию не могу попасть на концерт становится\n",
      "\n",
      "как побороть панику и боязнь людей когда идёшь мимо очереди и ищешь в ней друзей\n",
      " в россию и не умереть от скуки где уадик болезни\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "  seed_text = corpus[randint(0,len(corpus))]\n",
    "  print(seed_text)\n",
    "  generated = generate_text(seed_text, 10,  max_sequence_len, model)\n",
    "  print(generated+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(seed_text, n_words, max_sequence_len, model):"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gk4z_gen_6_TOKENS10-20PADDING.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
