{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41734"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = open('friends_questions_short.txt', encoding='UTF-8').readlines()\n",
    "answers = list()\n",
    "with open ('friends_answers_short.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for answer in f.readlines():\n",
    "        answers.append( '<START> ' + answer.strip() + ' <END>' ) \n",
    "combined_list = questions+answers\n",
    "len(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20867\n",
      "20867\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(combined_list ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4907"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = tokenizer.word_index\n",
    "num_tokens = len(word_dict )+1\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer_friends_short.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question max length is 16\n"
     ]
    }
   ],
   "source": [
    "tokenized_questions = tokenizer.texts_to_sequences(questions) \n",
    "length_list = list()\n",
    "for token_seq in tokenized_questions:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_input_length = np.array( length_list ).max()\n",
    "print( 'Question max length is {}'.format( max_input_length ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input data shape -> (20867, 16)\n"
     ]
    }
   ],
   "source": [
    "padded_questions = pad_sequences( tokenized_questions , maxlen=max_input_length , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('encoder_input_data_friends_short.npy', encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers max length is 19\n",
      "Decoder input data shape -> (20867, 19)\n"
     ]
    }
   ],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences( answers ) \n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_answers:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_output_length = np.array( length_list ).max()\n",
    "print( 'Answers max length is {}'.format( max_output_length ))\n",
    "\n",
    "padded_answers = pad_sequences( tokenized_answers , maxlen=max_output_length, padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('decoder_input_data_friends_short.npy', decoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target data shape -> (20867, 19, 4907)\n"
     ]
    }
   ],
   "source": [
    "decoder_target_data = list()\n",
    "for token_seq in tokenized_answers:\n",
    "    decoder_target_data.append( token_seq[ 1 : ] ) # убираем старт\n",
    "\n",
    "    \n",
    "padded_answers = pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , num_tokens )\n",
    "decoder_target_data = np.array( onehot_answers )\n",
    "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('decoder_target_data_friends_short.npy', decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pickle.load( open('tokenizer_friends_short.pkl' , 'rb'))\n",
    "\n",
    "num_tokens = len( tokenizer.word_index )\n",
    "word_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = num_tokens  # Adding again 1 because of reserved 0 index\n",
    "embedding_matrix_glove = np.zeros((vocab_size, 200)) # берем эмбеддинг размерностью 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\septo\\\\Lera\\\\glove\\\\glove.6B.200d.txt', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in tokenizer.word_index:\n",
    "                idx = tokenizer.word_index[word] \n",
    "                embedding_matrix_glove[idx] = np.array(\n",
    "                    vector, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4907, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37830001,  0.039647  ,  0.36320001, -0.098111  , -0.82511997,\n",
       "        0.020173  , -0.86044002,  0.30083999, -0.25029001,  0.86778003,\n",
       "       -0.089421  , -0.029677  , -0.060158  ,  0.15734001, -0.13674   ,\n",
       "        0.64468002, -1.03859997,  0.26578999,  0.53983998, -0.37524   ,\n",
       "        0.51603001,  0.55084997, -0.15421   , -0.13323   ,  0.093934  ,\n",
       "       -0.040462  , -0.097016  ,  0.12977999, -0.55970001, -0.34797999,\n",
       "       -0.10582   , -0.13418999, -0.26113999,  0.24352001, -0.41727999,\n",
       "        0.15101001, -0.24849001, -0.77275997, -0.75265002,  0.42407   ,\n",
       "       -1.03789997,  0.39776999,  0.21489   ,  0.22041   ,  0.0040744 ,\n",
       "        0.0086975 ,  0.54936999, -0.026504  , -0.63075   ,  0.13698   ,\n",
       "        0.79771   ,  0.029406  ,  0.54597998, -0.11041   , -0.32098001,\n",
       "        0.31044999, -0.03568   , -0.30546001,  0.41762999, -0.26973999,\n",
       "        0.53738999,  0.15655001,  0.18486001,  0.28977999, -0.76911002,\n",
       "       -0.65816998, -0.22201   , -0.28143001,  1.36210001, -0.42743999,\n",
       "        0.65957999, -1.14160001, -0.23834001, -0.22775   ,  0.2199    ,\n",
       "       -1.01390004, -0.82389998,  0.38646999, -0.19328   , -0.19812   ,\n",
       "        0.71619999,  0.68785   , -0.35416001,  0.30574   , -0.78860003,\n",
       "       -0.68651003,  0.13441999, -0.38104999, -0.32108   , -0.42188999,\n",
       "        0.51986998,  0.0076439 , -0.082932  , -0.34511   , -0.050649  ,\n",
       "       -0.039458  , -0.33941001,  0.058317  , -0.44126999, -0.12638   ,\n",
       "       -0.062843  ,  0.38856   ,  0.37661999,  0.53750002,  0.87941998,\n",
       "       -0.60896999, -0.56569999,  1.44420004, -0.44487   ,  0.30173001,\n",
       "        0.36423001,  1.00849998,  0.62097001,  0.39078   , -0.5546    ,\n",
       "       -0.2949    ,  0.60191   , -0.015965  , -0.86181998,  0.57927001,\n",
       "       -0.067419  ,  0.55724001, -0.10393   ,  0.56108999,  0.43768999,\n",
       "       -1.12179995,  0.25971001, -0.027784  ,  0.87682003, -1.09609997,\n",
       "       -0.48754999,  0.049957  , -0.43604001,  0.50518   , -0.45354   ,\n",
       "        0.076477  , -0.10272   ,  0.54289001,  0.32361001, -0.33250001,\n",
       "       -0.039745  ,  0.12659   , -0.040961  , -0.092605  ,  0.64696002,\n",
       "       -0.56863999,  0.68892998, -0.73271   , -0.29243001, -0.040692  ,\n",
       "        0.28762001,  0.74348998, -0.57534999,  0.57353002,  0.34391999,\n",
       "        0.80809999, -0.21927001,  0.11817   , -0.63804001, -0.17816   ,\n",
       "       -0.16068   , -0.22488999, -0.50191998, -0.99043   , -0.40847999,\n",
       "       -0.40752   ,  0.24596   ,  0.73246998, -0.20644   , -1.19889998,\n",
       "        0.044698  ,  0.46098   ,  1.1918    ,  0.14018001, -0.28049001,\n",
       "       -0.053451  , -1.47749996, -0.39535999, -0.021435  ,  0.43166   ,\n",
       "        1.0582    , -0.41494   , -0.2032    , -0.34244001,  0.61409998,\n",
       "       -0.46707001, -0.18803   ,  0.67084998,  0.81989998, -0.57514   ,\n",
       "       -0.61562997, -0.77809   ,  0.94871002, -0.19583   , -0.34362   ,\n",
       "       -0.12071   , -0.07393   , -0.018783  ,  0.31959   ,  0.54170001])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4489"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix_glove, axis=1))\n",
    "nonzero_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix_friends_short.npy', embedding_matrix_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
