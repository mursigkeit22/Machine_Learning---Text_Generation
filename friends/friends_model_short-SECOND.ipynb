{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "print( tf.VERSION )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of question is 16\n",
      "Max length of answer is 19\n",
      "(20867, 16)\n",
      "(20867, 19)\n",
      "(20867, 19, 4907)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.load( 'encoder_input_data_friends_short.npy' )\n",
    "decoder_input_data = np.load( 'decoder_input_data_friends_short.npy' )\n",
    "decoder_target_data = np.load( 'decoder_target_data_friends_short.npy' )\n",
    "\n",
    "embedding_matrix = np.load('embedding_matrix_friends_short.npy' ) \n",
    "\n",
    "tokenizer = pickle.load( open('tokenizer_friends_short.pkl' , 'rb'))\n",
    "\n",
    "num_tokens = len( tokenizer.word_index )+1\n",
    "word_dict = tokenizer.word_index\n",
    "\n",
    "max_question_len = encoder_input_data.shape[1]\n",
    "max_answer_len = decoder_input_data.shape[1]\n",
    "\n",
    "print( 'Max length of question is {}'.format( max_question_len) )\n",
    "print( 'Max length of answer is {}'.format( max_answer_len) )\n",
    "\n",
    "print( encoder_input_data.shape )\n",
    "print( decoder_input_data.shape )\n",
    "print( decoder_target_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    981400      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    981400      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 4907)   986307      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,590,707\n",
      "Trainable params: 3,590,707\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True , weights=[embedding_matrix] ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True, weights=[embedding_matrix]) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.load_weights('weights-friends_short-199-0.7454.hdf5')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-friends_short-SECOND-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1,  min_lr=0.000001, verbose=1,cooldown=1)\n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7604\n",
      "Epoch 00001: loss improved from inf to 0.76035, saving model to weights-friends_short-SECOND-01-0.7604.hdf5\n",
      "20867/20867 [==============================] - 45s 2ms/sample - loss: 0.7604\n",
      "Epoch 2/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7534\n",
      "Epoch 00002: loss improved from 0.76035 to 0.75365, saving model to weights-friends_short-SECOND-02-0.7537.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7537\n",
      "Epoch 3/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7492\n",
      "Epoch 00003: loss improved from 0.75365 to 0.74917, saving model to weights-friends_short-SECOND-03-0.7492.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7492\n",
      "Epoch 4/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7474\n",
      "Epoch 00004: loss improved from 0.74917 to 0.74755, saving model to weights-friends_short-SECOND-04-0.7476.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7476\n",
      "Epoch 5/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7450\n",
      "Epoch 00005: loss improved from 0.74755 to 0.74506, saving model to weights-friends_short-SECOND-05-0.7451.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7451\n",
      "Epoch 6/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7433\n",
      "Epoch 00006: loss improved from 0.74506 to 0.74363, saving model to weights-friends_short-SECOND-06-0.7436.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.7436\n",
      "Epoch 7/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7403\n",
      "Epoch 00007: loss improved from 0.74363 to 0.74039, saving model to weights-friends_short-SECOND-07-0.7404.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7404\n",
      "Epoch 8/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7393\n",
      "Epoch 00008: loss improved from 0.74039 to 0.73937, saving model to weights-friends_short-SECOND-08-0.7394.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7394\n",
      "Epoch 9/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7374\n",
      "Epoch 00009: loss improved from 0.73937 to 0.73745, saving model to weights-friends_short-SECOND-09-0.7374.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7374\n",
      "Epoch 10/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7366\n",
      "Epoch 00010: loss improved from 0.73745 to 0.73685, saving model to weights-friends_short-SECOND-10-0.7368.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7368\n",
      "Epoch 11/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7381\n",
      "Epoch 00011: loss did not improve from 0.73685\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7379\n",
      "Epoch 12/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6792\n",
      "Epoch 00012: loss improved from 0.73685 to 0.67929, saving model to weights-friends_short-SECOND-12-0.6793.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6793\n",
      "Epoch 13/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6510\n",
      "Epoch 00013: loss improved from 0.67929 to 0.65068, saving model to weights-friends_short-SECOND-13-0.6507.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6507\n",
      "Epoch 14/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6363\n",
      "Epoch 00014: loss improved from 0.65068 to 0.63613, saving model to weights-friends_short-SECOND-14-0.6361.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6361\n",
      "Epoch 15/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6295\n",
      "Epoch 00015: loss improved from 0.63613 to 0.62947, saving model to weights-friends_short-SECOND-15-0.6295.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6295\n",
      "Epoch 16/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6236\n",
      "Epoch 00016: loss improved from 0.62947 to 0.62345, saving model to weights-friends_short-SECOND-16-0.6234.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6234\n",
      "Epoch 17/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6184\n",
      "Epoch 00017: loss improved from 0.62345 to 0.61831, saving model to weights-friends_short-SECOND-17-0.6183.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6183\n",
      "Epoch 18/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6146\n",
      "Epoch 00018: loss improved from 0.61831 to 0.61463, saving model to weights-friends_short-SECOND-18-0.6146.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6146\n",
      "Epoch 19/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6119\n",
      "Epoch 00019: loss improved from 0.61463 to 0.61185, saving model to weights-friends_short-SECOND-19-0.6119.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6119\n",
      "Epoch 20/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6091\n",
      "Epoch 00020: loss improved from 0.61185 to 0.60895, saving model to weights-friends_short-SECOND-20-0.6090.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6090\n",
      "Epoch 21/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6071\n",
      "Epoch 00021: loss improved from 0.60895 to 0.60732, saving model to weights-friends_short-SECOND-21-0.6073.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6073\n",
      "Epoch 22/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6058\n",
      "Epoch 00022: loss improved from 0.60732 to 0.60560, saving model to weights-friends_short-SECOND-22-0.6056.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6056\n",
      "Epoch 23/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6033\n",
      "Epoch 00023: loss improved from 0.60560 to 0.60351, saving model to weights-friends_short-SECOND-23-0.6035.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6035\n",
      "Epoch 24/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6022\n",
      "Epoch 00024: loss improved from 0.60351 to 0.60213, saving model to weights-friends_short-SECOND-24-0.6021.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6021\n",
      "Epoch 25/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.6004\n",
      "Epoch 00025: loss improved from 0.60213 to 0.60043, saving model to weights-friends_short-SECOND-25-0.6004.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.6004\n",
      "Epoch 26/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5996\n",
      "Epoch 00026: loss improved from 0.60043 to 0.59949, saving model to weights-friends_short-SECOND-26-0.5995.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5995\n",
      "Epoch 27/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5975\n",
      "Epoch 00027: loss improved from 0.59949 to 0.59758, saving model to weights-friends_short-SECOND-27-0.5976.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5976\n",
      "Epoch 28/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5969\n",
      "Epoch 00028: loss improved from 0.59758 to 0.59688, saving model to weights-friends_short-SECOND-28-0.5969.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5969\n",
      "Epoch 29/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5955\n",
      "Epoch 00029: loss improved from 0.59688 to 0.59544, saving model to weights-friends_short-SECOND-29-0.5954.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5954\n",
      "Epoch 30/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5951\n",
      "Epoch 00030: loss improved from 0.59544 to 0.59487, saving model to weights-friends_short-SECOND-30-0.5949.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5949\n",
      "Epoch 31/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5936\n",
      "Epoch 00031: loss improved from 0.59487 to 0.59347, saving model to weights-friends_short-SECOND-31-0.5935.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5935\n",
      "Epoch 32/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5943\n",
      "Epoch 00032: loss did not improve from 0.59347\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5943\n",
      "Epoch 33/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5811\n",
      "Epoch 00033: loss improved from 0.59347 to 0.58108, saving model to weights-friends_short-SECOND-33-0.5811.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5811\n",
      "Epoch 34/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5785\n",
      "Epoch 00034: loss improved from 0.58108 to 0.57814, saving model to weights-friends_short-SECOND-34-0.5781.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5781\n",
      "Epoch 35/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5775\n",
      "Epoch 00035: loss improved from 0.57814 to 0.57754, saving model to weights-friends_short-SECOND-35-0.5775.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5775\n",
      "Epoch 36/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5771\n",
      "Epoch 00036: loss improved from 0.57754 to 0.57700, saving model to weights-friends_short-SECOND-36-0.5770.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5770\n",
      "Epoch 37/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5768\n",
      "Epoch 00037: loss improved from 0.57700 to 0.57680, saving model to weights-friends_short-SECOND-37-0.5768.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5768\n",
      "Epoch 38/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5755\n",
      "Epoch 00038: loss improved from 0.57680 to 0.57556, saving model to weights-friends_short-SECOND-38-0.5756.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5756\n",
      "Epoch 39/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5745\n",
      "Epoch 00039: loss improved from 0.57556 to 0.57454, saving model to weights-friends_short-SECOND-39-0.5745.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5745\n",
      "Epoch 40/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5756\n",
      "Epoch 00040: loss did not improve from 0.57454\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5757\n",
      "Epoch 41/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5718\n",
      "Epoch 00041: loss improved from 0.57454 to 0.57141, saving model to weights-friends_short-SECOND-41-0.5714.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5714\n",
      "Epoch 42/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5727\n",
      "Epoch 00042: loss did not improve from 0.57141\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5726\n",
      "Epoch 43/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5718\n",
      "Epoch 00043: loss did not improve from 0.57141\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5714\n",
      "Epoch 44/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5713\n",
      "Epoch 00044: loss improved from 0.57141 to 0.57132, saving model to weights-friends_short-SECOND-44-0.5713.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5713\n",
      "Epoch 45/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00045: loss improved from 0.57132 to 0.57032, saving model to weights-friends_short-SECOND-45-0.5703.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 46/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5713\n",
      "Epoch 00046: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5714\n",
      "Epoch 47/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5715\n",
      "Epoch 00047: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5713\n",
      "Epoch 48/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5713\n",
      "Epoch 00048: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5712\n",
      "Epoch 49/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5711\n",
      "Epoch 00049: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5710\n",
      "Epoch 50/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5712\n",
      "Epoch 00050: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5710\n",
      "Epoch 51/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5713\n",
      "Epoch 00051: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5712\n",
      "Epoch 52/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5712\n",
      "Epoch 00052: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5713\n",
      "Epoch 53/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00053: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 54/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00054: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 55/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5718\n",
      "Epoch 00055: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5717\n",
      "Epoch 56/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00056: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 57/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00057: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 58/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5720\n",
      "Epoch 00058: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5718\n",
      "Epoch 59/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5712\n",
      "Epoch 00059: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5713\n",
      "Epoch 60/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5716\n",
      "Epoch 00060: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5717\n",
      "Epoch 61/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00061: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 62/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5718\n",
      "Epoch 00062: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5718\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00063: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 64/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00064: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 65/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00065: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 66/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5711\n",
      "Epoch 00066: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5711\n",
      "Epoch 67/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00067: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 68/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00068: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 69/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00069: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 70/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5712\n",
      "Epoch 00070: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5710\n",
      "Epoch 71/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00071: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5710\n",
      "Epoch 72/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00072: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 73/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00073: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5709\n",
      "Epoch 74/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00074: loss did not improve from 0.57032\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5709\n",
      "Epoch 75/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00075: loss improved from 0.57032 to 0.57023, saving model to weights-friends_short-SECOND-75-0.5702.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 76/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00076: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5709\n",
      "Epoch 77/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00077: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 78/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00078: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 79/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5711\n",
      "Epoch 00079: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5711\n",
      "Epoch 80/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00080: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 81/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5710\n",
      "Epoch 00081: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 82/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5711\n",
      "Epoch 00082: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5711\n",
      "Epoch 83/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5709\n",
      "Epoch 00083: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 84/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00084: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 85/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00085: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 86/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00086: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 87/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00087: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 88/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00088: loss did not improve from 0.57023\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 89/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00089: loss improved from 0.57023 to 0.57020, saving model to weights-friends_short-SECOND-89-0.5702.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 90/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5711\n",
      "Epoch 00090: loss did not improve from 0.57020\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 91/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00091: loss did not improve from 0.57020\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 92/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5712\n",
      "Epoch 00092: loss did not improve from 0.57020\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.5713\n",
      "Epoch 93/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00093: loss did not improve from 0.57020\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 94/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5710\n",
      "Epoch 00094: loss did not improve from 0.57020\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5711\n",
      "Epoch 95/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00095: loss did not improve from 0.57020\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 96/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00096: loss did not improve from 0.57020\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 97/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00097: loss improved from 0.57020 to 0.57015, saving model to weights-friends_short-SECOND-97-0.5701.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5701\n",
      "Epoch 98/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00098: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 99/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5714\n",
      "Epoch 00099: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5712\n",
      "Epoch 100/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00100: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 101/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00101: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00102: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 103/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00103: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 104/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00104: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 105/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00105: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 106/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00106: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 107/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00107: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 108/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5715\n",
      "Epoch 00108: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5715\n",
      "Epoch 109/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00109: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 110/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00110: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 111/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00111: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5710\n",
      "Epoch 112/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00112: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 113/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00113: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5702\n",
      "Epoch 114/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00114: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5706\n",
      "Epoch 115/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00115: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 116/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00116: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 117/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00117: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5710\n",
      "Epoch 118/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00118: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 119/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00119: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 120/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00120: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 121/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00121: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 122/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00122: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5709\n",
      "Epoch 123/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5699\n",
      "Epoch 00123: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 124/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00124: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 125/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00125: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 126/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00126: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 127/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00127: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 128/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00128: loss did not improve from 0.57015\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 129/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5700\n",
      "Epoch 00129: loss improved from 0.57015 to 0.56980, saving model to weights-friends_short-SECOND-129-0.5698.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5698\n",
      "Epoch 130/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00130: loss did not improve from 0.56980\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 131/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00131: loss did not improve from 0.56980\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 132/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5696\n",
      "Epoch 00132: loss improved from 0.56980 to 0.56946, saving model to weights-friends_short-SECOND-132-0.5695.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5695\n",
      "Epoch 133/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00133: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 134/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00134: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 135/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00135: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.5707\n",
      "Epoch 136/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5698\n",
      "Epoch 00136: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5698\n",
      "Epoch 137/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00137: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 138/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00138: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 139/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00139: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5702\n",
      "Epoch 140/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5695\n",
      "Epoch 00140: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5706\n",
      "Epoch 00141: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 142/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00142: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 143/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00143: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5708\n",
      "Epoch 144/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 00144: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5698\n",
      "Epoch 145/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00145: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5700\n",
      "Epoch 146/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00146: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5700\n",
      "Epoch 147/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5692\n",
      "Epoch 00147: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5695\n",
      "Epoch 148/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5709\n",
      "Epoch 00148: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5707\n",
      "Epoch 149/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00149: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 150/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00150: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5701\n",
      "Epoch 151/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 00151: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 152/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5696\n",
      "Epoch 00152: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5697\n",
      "Epoch 153/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00153: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5701\n",
      "Epoch 154/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5699\n",
      "Epoch 00154: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5699\n",
      "Epoch 155/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5693\n",
      "Epoch 00155: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5696\n",
      "Epoch 156/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00156: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 157/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00157: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5701\n",
      "Epoch 158/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5699\n",
      "Epoch 00158: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5698\n",
      "Epoch 159/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 00159: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5696\n",
      "Epoch 160/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5700\n",
      "Epoch 00160: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5700\n",
      "Epoch 161/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00161: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 162/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00162: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.5703\n",
      "Epoch 163/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 00163: loss did not improve from 0.56946\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5703\n",
      "Epoch 164/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5693\n",
      "Epoch 00164: loss improved from 0.56946 to 0.56938, saving model to weights-friends_short-SECOND-164-0.5694.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5694\n",
      "Epoch 165/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5698\n",
      "Epoch 00165: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5701\n",
      "Epoch 166/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00166: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5700\n",
      "Epoch 167/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5698\n",
      "Epoch 00167: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5699\n",
      "Epoch 168/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 00168: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5706\n",
      "Epoch 169/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5700\n",
      "Epoch 00169: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5699\n",
      "Epoch 170/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00170: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5704\n",
      "Epoch 171/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 00171: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5696\n",
      "Epoch 172/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5694\n",
      "Epoch 00172: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5695\n",
      "Epoch 173/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5699\n",
      "Epoch 00173: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 174/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00174: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5699\n",
      "Epoch 175/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5699\n",
      "Epoch 00175: loss did not improve from 0.56938\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5700\n",
      "Epoch 176/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5689\n",
      "Epoch 00176: loss improved from 0.56938 to 0.56921, saving model to weights-friends_short-SECOND-176-0.5692.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5692\n",
      "Epoch 177/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 00177: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5694\n",
      "Epoch 178/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 00178: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5700\n",
      "Epoch 179/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5698\n",
      "Epoch 00179: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00180: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5704\n",
      "Epoch 181/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5709\n",
      "Epoch 00181: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5709\n",
      "Epoch 182/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5694\n",
      "Epoch 00182: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5695\n",
      "Epoch 183/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5694\n",
      "Epoch 00183: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.5696\n",
      "Epoch 184/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00184: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.5699\n",
      "Epoch 185/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5698\n",
      "Epoch 00185: loss did not improve from 0.56921\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5698\n",
      "Epoch 186/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5685\n",
      "Epoch 00186: loss improved from 0.56921 to 0.56844, saving model to weights-friends_short-SECOND-186-0.5684.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5684\n",
      "Epoch 187/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5692\n",
      "Epoch 00187: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5692\n",
      "Epoch 188/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00188: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5701\n",
      "Epoch 189/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5696\n",
      "Epoch 00189: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5696\n",
      "Epoch 190/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5694\n",
      "Epoch 00190: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5695\n",
      "Epoch 191/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5693\n",
      "Epoch 00191: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5693\n",
      "Epoch 192/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5695\n",
      "Epoch 00192: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5696\n",
      "Epoch 193/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 00193: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5700\n",
      "Epoch 194/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 00194: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5703\n",
      "Epoch 195/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 00195: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5699\n",
      "Epoch 196/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5694\n",
      "Epoch 00196: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5696\n",
      "Epoch 197/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 00197: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5695\n",
      "Epoch 198/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 00198: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5697\n",
      "Epoch 199/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 00199: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5705\n",
      "Epoch 200/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.5694\n",
      "Epoch 00200: loss did not improve from 0.56844\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.5698\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=200, callbacks=callbacks_list ) \n",
    "model.save( 'model_friends_short-SECOND_epochs200.h5' ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokens_list.append( word_dict[ word ] ) \n",
    "        except:\n",
    "            continue\n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_question_len , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : how you doing\n",
      " yeah yeah i think it looks pretty good end\n",
      "Enter question : how much\n",
      " all right i'm gonna go take a and take it's on the yeah end\n",
      "Enter question : well\n",
      " yeah end\n",
      "Enter question : i went out last night\n",
      " all right good think i'm is that i'm gotta get going with the are believe the guy you're your apartment\n",
      "Enter question : how\n",
      " well i was know and i had a my something end\n",
      "Enter question : stupid\n",
      " you got it end\n",
      "Enter question : get out\n",
      " yeah end\n",
      "Enter question : yeah what\n",
      " what end\n",
      "Enter question : what what\n",
      " you guys were right end\n",
      "Enter question : that's right! smart bot!\n",
      " oh can't you you know about end\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_answer_len:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
