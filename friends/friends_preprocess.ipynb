{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48760"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = open('friends_questions.txt', encoding='UTF-8').readlines()\n",
    "answers = list()\n",
    "with open ('friends_answers.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for answer in f.readlines():\n",
    "        answers.append( '<START> ' + answer.strip() + ' <END>' ) \n",
    "combined_list = questions+answers\n",
    "len(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24380\n",
      "24380\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(combined_list ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8871"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = tokenizer.word_index\n",
    "num_tokens = len(word_dict )+1\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer_friends.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question max length is 16\n"
     ]
    }
   ],
   "source": [
    "tokenized_questions = tokenizer.texts_to_sequences(questions) \n",
    "length_list = list()\n",
    "for token_seq in tokenized_questions:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_input_length = np.array( length_list ).max()\n",
    "print( 'Question max length is {}'.format( max_input_length ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input data shape -> (24380, 16)\n"
     ]
    }
   ],
   "source": [
    "padded_questions = pad_sequences( tokenized_questions , maxlen=max_input_length , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('encoder_input_data_friends.npy', encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers max length is 19\n",
      "Decoder input data shape -> (24380, 19)\n"
     ]
    }
   ],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences( answers ) \n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_answers:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_output_length = np.array( length_list ).max()\n",
    "print( 'Answers max length is {}'.format( max_output_length ))\n",
    "\n",
    "padded_answers = pad_sequences( tokenized_answers , maxlen=max_output_length, padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('decoder_input_data_friends.npy', decoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target data shape -> (24380, 19, 8871)\n"
     ]
    }
   ],
   "source": [
    "decoder_target_data = list()\n",
    "for token_seq in tokenized_answers:\n",
    "    decoder_target_data.append( token_seq[ 1 : ] ) # убираем старт\n",
    "\n",
    "    \n",
    "padded_answers = pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , num_tokens )\n",
    "decoder_target_data = np.array( onehot_answers )\n",
    "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('decoder_target_data_friends.npy', decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = pickle.load( open('tokenizer_friends.pkl' , 'rb'))\n",
    "\n",
    "# num_tokens = len( tokenizer.word_index )\n",
    "# word_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = num_tokens  # Adding again 1 because of reserved 0 index # already done\n",
    "embedding_matrix_glove = np.zeros((vocab_size, 200)) # берем эмбеддинг размерностью 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\septo\\\\Lera\\\\glove\\\\glove.6B.200d.txt', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in tokenizer.word_index:\n",
    "                idx = tokenizer.word_index[word] \n",
    "                embedding_matrix_glove[idx] = np.array(\n",
    "                    vector, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8871, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.73459983e-01,  5.41700006e-01, -2.34770000e-01, -3.62399995e-01,\n",
       "        4.03699994e-01,  1.13860004e-01, -4.49330002e-01, -3.09909999e-01,\n",
       "       -5.34110004e-03,  5.84259987e-01, -2.59559993e-02,  4.93930012e-01,\n",
       "       -3.72090004e-02, -2.84280002e-01,  9.76959988e-02, -4.89069998e-01,\n",
       "        2.60269996e-02,  3.76489997e-01,  5.77879995e-02, -4.68070000e-01,\n",
       "        8.12880024e-02,  3.28250003e+00, -6.36900008e-01,  3.79559994e-01,\n",
       "        3.81670007e-03,  9.36070010e-02, -1.28549993e-01,  1.73800007e-01,\n",
       "        1.05219997e-01,  2.86480010e-01,  2.10889995e-01, -4.70759988e-01,\n",
       "        2.77330000e-02, -1.98029995e-01,  7.63280019e-02, -8.46289992e-01,\n",
       "       -7.97079980e-01, -3.87430012e-01, -3.04220002e-02, -2.68489987e-01,\n",
       "        4.85850006e-01,  1.28950000e-01,  3.83540004e-01,  3.87219995e-01,\n",
       "       -3.85239989e-01,  1.90750003e-01,  4.89980012e-01,  1.32780001e-01,\n",
       "        1.07920002e-02,  2.67699987e-01,  1.78120002e-01, -1.14330001e-01,\n",
       "       -3.34939986e-01,  8.73059988e-01,  7.58750021e-01, -3.03779989e-01,\n",
       "       -1.56259999e-01,  1.20850001e-03,  2.33219996e-01,  2.79529989e-01,\n",
       "       -1.84939995e-01, -1.41460001e-01, -1.89689994e-01, -3.83859985e-02,\n",
       "        3.58740002e-01,  6.55129999e-02,  6.05649985e-02,  6.63389981e-01,\n",
       "       -8.32519978e-02,  6.51630014e-02,  5.17610013e-01,  1.61709994e-01,\n",
       "        4.60110009e-01,  1.63880005e-01, -1.23989999e-01,  3.11219990e-01,\n",
       "       -1.54119998e-01, -1.09169997e-01, -4.25509989e-01,  1.14179999e-01,\n",
       "        2.51370013e-01, -5.61579987e-02, -2.59270012e-01,  2.81630009e-01,\n",
       "       -1.80939995e-02,  1.60650000e-01, -4.85060006e-01, -9.89030004e-01,\n",
       "        2.50220001e-01, -1.67359993e-01,  4.14739996e-01,  1.77010000e-01,\n",
       "        4.24070001e-01,  1.10880002e-01, -1.83599994e-01, -1.24100000e-01,\n",
       "       -3.47799987e-01,  9.90779996e-02, -2.23810002e-01, -1.12450004e-01,\n",
       "       -2.11559996e-01,  3.07060010e-03, -2.36070007e-01,  2.72610001e-02,\n",
       "        3.64300013e-01,  3.99219990e-02, -1.83689997e-01,  1.22660005e+00,\n",
       "       -7.76400030e-01, -6.62249982e-01,  1.57239996e-02, -1.49690002e-01,\n",
       "        8.46489966e-02,  2.68139988e-01, -1.67649999e-01, -3.19420010e-01,\n",
       "        2.84940004e-01, -7.00000003e-02,  1.20099997e-02, -1.22189999e-01,\n",
       "        5.63099980e-01, -3.19999993e-01,  5.01089990e-01, -1.02090001e-01,\n",
       "        4.65750009e-01, -7.15420008e-01,  1.72930002e-01,  5.82589984e-01,\n",
       "        7.83839971e-02, -3.38440016e-02, -2.51289994e-01,  3.65029991e-01,\n",
       "        3.15780006e-02, -6.57779992e-01,  5.47499992e-02,  8.71890008e-01,\n",
       "        1.24550000e-01, -4.58770007e-01, -2.69650012e-01, -4.67790008e-01,\n",
       "       -2.85780011e-03,  1.78100005e-01,  6.39689982e-01,  1.39950007e-01,\n",
       "        9.75960016e-01,  1.18359998e-01, -6.39039993e-01, -1.54159993e-01,\n",
       "        6.52619973e-02,  2.43290007e-01,  6.64759994e-01,  2.50690013e-01,\n",
       "       -1.02519996e-01, -3.28390002e-01, -8.55590031e-02, -1.27739999e-02,\n",
       "       -1.94309995e-01,  5.61389983e-01, -3.57329994e-01, -2.03439996e-01,\n",
       "       -1.24130003e-01, -3.44309986e-01, -2.32960001e-01, -2.11870000e-01,\n",
       "        8.53869990e-02,  7.00630024e-02, -1.98029995e-01, -2.60230005e-02,\n",
       "       -3.90370011e-01,  8.00019979e-01,  4.05770004e-01, -7.98629969e-02,\n",
       "        3.52629989e-01, -3.40429991e-01,  3.96759987e-01,  2.28619993e-01,\n",
       "       -3.50279987e-01, -4.73439991e-01,  5.97419977e-01, -1.16570003e-01,\n",
       "        1.05519998e+00, -4.15699989e-01, -8.05519968e-02, -5.65709993e-02,\n",
       "       -1.66219994e-01,  1.92739993e-01, -9.51749980e-02, -2.07810000e-01,\n",
       "        1.56200007e-01,  5.02309985e-02, -2.79150009e-01,  4.37420011e-01,\n",
       "       -3.12370002e-01,  1.31940007e-01, -3.32780004e-01,  1.88769996e-01,\n",
       "       -2.34219998e-01,  5.44179976e-01, -2.30690002e-01,  3.49469990e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7767"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix_glove, axis=1))\n",
    "nonzero_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix_friends.npy', embedding_matrix_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
