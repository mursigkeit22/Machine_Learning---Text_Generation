{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print( tf.VERSION )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of question is 16\n",
      "Max length of answer is 19\n",
      "(20867, 16)\n",
      "(20867, 19)\n",
      "(20867, 19, 4907)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.load( 'encoder_input_data_friends_short.npy' )\n",
    "decoder_input_data = np.load( 'decoder_input_data_friends_short.npy' )\n",
    "decoder_target_data = np.load( 'decoder_target_data_friends_short.npy' )\n",
    "\n",
    "embedding_matrix = np.load('embedding_matrix_friends_short.npy' ) \n",
    "\n",
    "tokenizer = pickle.load( open('tokenizer_friends_short.pkl' , 'rb'))\n",
    "\n",
    "num_tokens = len( tokenizer.word_index )+1\n",
    "word_dict = tokenizer.word_index\n",
    "\n",
    "max_question_len = encoder_input_data.shape[1]\n",
    "max_answer_len = decoder_input_data.shape[1]\n",
    "\n",
    "print( 'Max length of question is {}'.format( max_question_len) )\n",
    "print( 'Max length of answer is {}'.format( max_answer_len) )\n",
    "\n",
    "print( encoder_input_data.shape )\n",
    "print( decoder_input_data.shape )\n",
    "print( decoder_target_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    981400      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    981400      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 4907)   986307      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,590,707\n",
      "Trainable params: 3,590,707\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True , weights=[embedding_matrix] ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True, weights=[embedding_matrix]) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-friends_short-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 4.2419\n",
      "Epoch 00001: loss improved from inf to 4.24152, saving model to weights-friends_short-01-4.2415.hdf5\n",
      "20867/20867 [==============================] - 45s 2ms/sample - loss: 4.2415\n",
      "Epoch 2/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.7500\n",
      "Epoch 00002: loss improved from 4.24152 to 3.74990, saving model to weights-friends_short-02-3.7499.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.7499\n",
      "Epoch 3/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.5849\n",
      "Epoch 00003: loss improved from 3.74990 to 3.58486, saving model to weights-friends_short-03-3.5849.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.5849\n",
      "Epoch 4/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.4704\n",
      "Epoch 00004: loss improved from 3.58486 to 3.47019, saving model to weights-friends_short-04-3.4702.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.4702\n",
      "Epoch 5/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.3699\n",
      "Epoch 00005: loss improved from 3.47019 to 3.36976, saving model to weights-friends_short-05-3.3698.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.3698\n",
      "Epoch 6/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.2795\n",
      "Epoch 00006: loss improved from 3.36976 to 3.27917, saving model to weights-friends_short-06-3.2792.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.2792\n",
      "Epoch 7/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.1992\n",
      "Epoch 00007: loss improved from 3.27917 to 3.19899, saving model to weights-friends_short-07-3.1990.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.1990\n",
      "Epoch 8/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.1149\n",
      "Epoch 00008: loss improved from 3.19899 to 3.11532, saving model to weights-friends_short-08-3.1153.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.1153\n",
      "Epoch 9/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 3.0284\n",
      "Epoch 00009: loss improved from 3.11532 to 3.02794, saving model to weights-friends_short-09-3.0279.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 3.0279\n",
      "Epoch 10/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.9446\n",
      "Epoch 00010: loss improved from 3.02794 to 2.94476, saving model to weights-friends_short-10-2.9448.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.9448\n",
      "Epoch 11/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.8592\n",
      "Epoch 00011: loss improved from 2.94476 to 2.85924, saving model to weights-friends_short-11-2.8592.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.8592\n",
      "Epoch 12/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.7752\n",
      "Epoch 00012: loss improved from 2.85924 to 2.77540, saving model to weights-friends_short-12-2.7754.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.7754\n",
      "Epoch 13/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.6973\n",
      "Epoch 00013: loss improved from 2.77540 to 2.69747, saving model to weights-friends_short-13-2.6975.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.6975\n",
      "Epoch 14/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.6184\n",
      "Epoch 00014: loss improved from 2.69747 to 2.61879, saving model to weights-friends_short-14-2.6188.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.6188\n",
      "Epoch 15/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.5399\n",
      "Epoch 00015: loss improved from 2.61879 to 2.54069, saving model to weights-friends_short-15-2.5407.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 2.5407\n",
      "Epoch 16/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.4573\n",
      "Epoch 00016: loss improved from 2.54069 to 2.45743, saving model to weights-friends_short-16-2.4574.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 2.4574\n",
      "Epoch 17/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.3778\n",
      "Epoch 00017: loss improved from 2.45743 to 2.37777, saving model to weights-friends_short-17-2.3778.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 2.3778\n",
      "Epoch 18/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.2985\n",
      "Epoch 00018: loss improved from 2.37777 to 2.29842, saving model to weights-friends_short-18-2.2984.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 2.2984\n",
      "Epoch 19/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.2273\n",
      "Epoch 00019: loss improved from 2.29842 to 2.22716, saving model to weights-friends_short-19-2.2272.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.2272\n",
      "Epoch 20/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.1574\n",
      "Epoch 00020: loss improved from 2.22716 to 2.15794, saving model to weights-friends_short-20-2.1579.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.1579\n",
      "Epoch 21/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.0925\n",
      "Epoch 00021: loss improved from 2.15794 to 2.09268, saving model to weights-friends_short-21-2.0927.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 2.0927\n",
      "Epoch 22/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 2.0276\n",
      "Epoch 00022: loss improved from 2.09268 to 2.02786, saving model to weights-friends_short-22-2.0279.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 2.0279\n",
      "Epoch 23/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.9671\n",
      "Epoch 00023: loss improved from 2.02786 to 1.96761, saving model to weights-friends_short-23-1.9676.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.9676\n",
      "Epoch 24/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.9062\n",
      "Epoch 00024: loss improved from 1.96761 to 1.90625, saving model to weights-friends_short-24-1.9062.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 1.9062\n",
      "Epoch 25/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.8550\n",
      "Epoch 00025: loss improved from 1.90625 to 1.85510, saving model to weights-friends_short-25-1.8551.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.8551\n",
      "Epoch 26/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.8086\n",
      "Epoch 00026: loss improved from 1.85510 to 1.80865, saving model to weights-friends_short-26-1.8086.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.8086\n",
      "Epoch 27/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.7628\n",
      "Epoch 00027: loss improved from 1.80865 to 1.76266, saving model to weights-friends_short-27-1.7627.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.7627\n",
      "Epoch 28/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.7225\n",
      "Epoch 00028: loss improved from 1.76266 to 1.72289, saving model to weights-friends_short-28-1.7229.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.7229\n",
      "Epoch 29/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.6838\n",
      "Epoch 00029: loss improved from 1.72289 to 1.68330, saving model to weights-friends_short-29-1.6833.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.6833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.6484\n",
      "Epoch 00030: loss improved from 1.68330 to 1.64884, saving model to weights-friends_short-30-1.6488.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.6488\n",
      "Epoch 31/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.6151\n",
      "Epoch 00031: loss improved from 1.64884 to 1.61503, saving model to weights-friends_short-31-1.6150.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.6150\n",
      "Epoch 32/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.5843\n",
      "Epoch 00032: loss improved from 1.61503 to 1.58404, saving model to weights-friends_short-32-1.5840.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.5840\n",
      "Epoch 33/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.5582\n",
      "Epoch 00033: loss improved from 1.58404 to 1.55859, saving model to weights-friends_short-33-1.5586.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.5586\n",
      "Epoch 34/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.5365\n",
      "Epoch 00034: loss improved from 1.55859 to 1.53653, saving model to weights-friends_short-34-1.5365.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.5365\n",
      "Epoch 35/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.5170\n",
      "Epoch 00035: loss improved from 1.53653 to 1.51743, saving model to weights-friends_short-35-1.5174.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.5174\n",
      "Epoch 36/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.4980\n",
      "Epoch 00036: loss improved from 1.51743 to 1.49803, saving model to weights-friends_short-36-1.4980.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 1.4980\n",
      "Epoch 37/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.4820\n",
      "Epoch 00037: loss improved from 1.49803 to 1.48196, saving model to weights-friends_short-37-1.4820.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 1.4820\n",
      "Epoch 38/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.4642\n",
      "Epoch 00038: loss improved from 1.48196 to 1.46401, saving model to weights-friends_short-38-1.4640.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 1.4640\n",
      "Epoch 39/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.4393\n",
      "Epoch 00039: loss improved from 1.46401 to 1.43964, saving model to weights-friends_short-39-1.4396.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.4396\n",
      "Epoch 40/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.4188\n",
      "Epoch 00040: loss improved from 1.43964 to 1.41908, saving model to weights-friends_short-40-1.4191.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.4191\n",
      "Epoch 41/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.3984\n",
      "Epoch 00041: loss improved from 1.41908 to 1.39847, saving model to weights-friends_short-41-1.3985.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.3985\n",
      "Epoch 42/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.3788\n",
      "Epoch 00042: loss improved from 1.39847 to 1.37918, saving model to weights-friends_short-42-1.3792.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.3792\n",
      "Epoch 43/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.3584\n",
      "Epoch 00043: loss improved from 1.37918 to 1.35847, saving model to weights-friends_short-43-1.3585.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.3585\n",
      "Epoch 44/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.3398\n",
      "Epoch 00044: loss improved from 1.35847 to 1.33997, saving model to weights-friends_short-44-1.3400.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.3400\n",
      "Epoch 45/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.3244\n",
      "Epoch 00045: loss improved from 1.33997 to 1.32472, saving model to weights-friends_short-45-1.3247.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.3247\n",
      "Epoch 46/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.3126\n",
      "Epoch 00046: loss improved from 1.32472 to 1.31238, saving model to weights-friends_short-46-1.3124.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.3124\n",
      "Epoch 47/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.3001\n",
      "Epoch 00047: loss improved from 1.31238 to 1.30032, saving model to weights-friends_short-47-1.3003.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.3003\n",
      "Epoch 48/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2904\n",
      "Epoch 00048: loss improved from 1.30032 to 1.29034, saving model to weights-friends_short-48-1.2903.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2903\n",
      "Epoch 49/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2791\n",
      "Epoch 00049: loss improved from 1.29034 to 1.27896, saving model to weights-friends_short-49-1.2790.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2790\n",
      "Epoch 50/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2650\n",
      "Epoch 00050: loss improved from 1.27896 to 1.26483, saving model to weights-friends_short-50-1.2648.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2648\n",
      "Epoch 51/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2505\n",
      "Epoch 00051: loss improved from 1.26483 to 1.25046, saving model to weights-friends_short-51-1.2505.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2505\n",
      "Epoch 52/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2430\n",
      "Epoch 00052: loss improved from 1.25046 to 1.24340, saving model to weights-friends_short-52-1.2434.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2434\n",
      "Epoch 53/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2351\n",
      "Epoch 00053: loss improved from 1.24340 to 1.23553, saving model to weights-friends_short-53-1.2355.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2355\n",
      "Epoch 54/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2277\n",
      "Epoch 00054: loss improved from 1.23553 to 1.22776, saving model to weights-friends_short-54-1.2278.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2278\n",
      "Epoch 55/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2232\n",
      "Epoch 00055: loss improved from 1.22776 to 1.22310, saving model to weights-friends_short-55-1.2231.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2231\n",
      "Epoch 56/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2141\n",
      "Epoch 00056: loss improved from 1.22310 to 1.21442, saving model to weights-friends_short-56-1.2144.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2144\n",
      "Epoch 57/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.2068\n",
      "Epoch 00057: loss improved from 1.21442 to 1.20659, saving model to weights-friends_short-57-1.2066.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.2066\n",
      "Epoch 58/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1974\n",
      "Epoch 00058: loss improved from 1.20659 to 1.19714, saving model to weights-friends_short-58-1.1971.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 1.1971\n",
      "Epoch 59/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1907\n",
      "Epoch 00059: loss improved from 1.19714 to 1.19073, saving model to weights-friends_short-59-1.1907.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1907\n",
      "Epoch 60/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1790\n",
      "Epoch 00060: loss improved from 1.19073 to 1.17890, saving model to weights-friends_short-60-1.1789.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1720\n",
      "Epoch 00061: loss improved from 1.17890 to 1.17194, saving model to weights-friends_short-61-1.1719.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1719\n",
      "Epoch 62/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1635\n",
      "Epoch 00062: loss improved from 1.17194 to 1.16331, saving model to weights-friends_short-62-1.1633.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1633\n",
      "Epoch 63/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1535\n",
      "Epoch 00063: loss improved from 1.16331 to 1.15349, saving model to weights-friends_short-63-1.1535.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1535\n",
      "Epoch 64/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1443\n",
      "Epoch 00064: loss improved from 1.15349 to 1.14415, saving model to weights-friends_short-64-1.1442.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1442\n",
      "Epoch 65/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1348\n",
      "Epoch 00065: loss improved from 1.14415 to 1.13520, saving model to weights-friends_short-65-1.1352.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 1.1352\n",
      "Epoch 66/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1255\n",
      "Epoch 00066: loss improved from 1.13520 to 1.12523, saving model to weights-friends_short-66-1.1252.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 1.1252\n",
      "Epoch 67/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1185\n",
      "Epoch 00067: loss improved from 1.12523 to 1.11865, saving model to weights-friends_short-67-1.1187.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1187\n",
      "Epoch 68/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.1085\n",
      "Epoch 00068: loss improved from 1.11865 to 1.10830, saving model to weights-friends_short-68-1.1083.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.1083\n",
      "Epoch 69/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0996\n",
      "Epoch 00069: loss improved from 1.10830 to 1.09964, saving model to weights-friends_short-69-1.0996.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0996\n",
      "Epoch 70/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0932\n",
      "Epoch 00070: loss improved from 1.09964 to 1.09320, saving model to weights-friends_short-70-1.0932.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0932\n",
      "Epoch 71/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0850\n",
      "Epoch 00071: loss improved from 1.09320 to 1.08539, saving model to weights-friends_short-71-1.0854.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0854\n",
      "Epoch 72/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0811\n",
      "Epoch 00072: loss improved from 1.08539 to 1.08097, saving model to weights-friends_short-72-1.0810.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0810\n",
      "Epoch 73/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0736\n",
      "Epoch 00073: loss improved from 1.08097 to 1.07328, saving model to weights-friends_short-73-1.0733.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0733\n",
      "Epoch 74/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0652\n",
      "Epoch 00074: loss improved from 1.07328 to 1.06522, saving model to weights-friends_short-74-1.0652.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0652\n",
      "Epoch 75/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0577\n",
      "Epoch 00075: loss improved from 1.06522 to 1.05825, saving model to weights-friends_short-75-1.0583.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0583\n",
      "Epoch 76/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0505\n",
      "Epoch 00076: loss improved from 1.05825 to 1.05026, saving model to weights-friends_short-76-1.0503.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0503\n",
      "Epoch 77/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0438\n",
      "Epoch 00077: loss improved from 1.05026 to 1.04372, saving model to weights-friends_short-77-1.0437.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0437\n",
      "Epoch 78/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0374\n",
      "Epoch 00078: loss improved from 1.04372 to 1.03753, saving model to weights-friends_short-78-1.0375.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0375\n",
      "Epoch 79/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0325\n",
      "Epoch 00079: loss improved from 1.03753 to 1.03250, saving model to weights-friends_short-79-1.0325.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0325\n",
      "Epoch 80/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0255\n",
      "Epoch 00080: loss improved from 1.03250 to 1.02569, saving model to weights-friends_short-80-1.0257.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0257\n",
      "Epoch 81/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0207\n",
      "Epoch 00081: loss improved from 1.02569 to 1.02073, saving model to weights-friends_short-81-1.0207.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0207\n",
      "Epoch 82/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0156\n",
      "Epoch 00082: loss improved from 1.02073 to 1.01528, saving model to weights-friends_short-82-1.0153.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0153\n",
      "Epoch 83/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0076\n",
      "Epoch 00083: loss improved from 1.01528 to 1.00752, saving model to weights-friends_short-83-1.0075.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0075\n",
      "Epoch 84/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 1.0019\n",
      "Epoch 00084: loss improved from 1.00752 to 1.00226, saving model to weights-friends_short-84-1.0023.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 1.0023\n",
      "Epoch 85/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9960\n",
      "Epoch 00085: loss improved from 1.00226 to 0.99588, saving model to weights-friends_short-85-0.9959.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9959\n",
      "Epoch 86/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9913\n",
      "Epoch 00086: loss improved from 0.99588 to 0.99173, saving model to weights-friends_short-86-0.9917.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.9917\n",
      "Epoch 87/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9873\n",
      "Epoch 00087: loss improved from 0.99173 to 0.98716, saving model to weights-friends_short-87-0.9872.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.9872\n",
      "Epoch 88/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9818\n",
      "Epoch 00088: loss improved from 0.98716 to 0.98174, saving model to weights-friends_short-88-0.9817.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.9817\n",
      "Epoch 89/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9757\n",
      "Epoch 00089: loss improved from 0.98174 to 0.97569, saving model to weights-friends_short-89-0.9757.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9757\n",
      "Epoch 90/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9709\n",
      "Epoch 00090: loss improved from 0.97569 to 0.97085, saving model to weights-friends_short-90-0.9709.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9709\n",
      "Epoch 91/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9651\n",
      "Epoch 00091: loss improved from 0.97085 to 0.96504, saving model to weights-friends_short-91-0.9650.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9579\n",
      "Epoch 00092: loss improved from 0.96504 to 0.95815, saving model to weights-friends_short-92-0.9582.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9582\n",
      "Epoch 93/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9544\n",
      "Epoch 00093: loss improved from 0.95815 to 0.95429, saving model to weights-friends_short-93-0.9543.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9543\n",
      "Epoch 94/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9487\n",
      "Epoch 00094: loss improved from 0.95429 to 0.94846, saving model to weights-friends_short-94-0.9485.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9485\n",
      "Epoch 95/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9438\n",
      "Epoch 00095: loss improved from 0.94846 to 0.94417, saving model to weights-friends_short-95-0.9442.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9442\n",
      "Epoch 96/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9385\n",
      "Epoch 00096: loss improved from 0.94417 to 0.93876, saving model to weights-friends_short-96-0.9388.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9388\n",
      "Epoch 97/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9380\n",
      "Epoch 00097: loss improved from 0.93876 to 0.93816, saving model to weights-friends_short-97-0.9382.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9382\n",
      "Epoch 98/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9316\n",
      "Epoch 00098: loss improved from 0.93816 to 0.93161, saving model to weights-friends_short-98-0.9316.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9316\n",
      "Epoch 99/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9275\n",
      "Epoch 00099: loss improved from 0.93161 to 0.92777, saving model to weights-friends_short-99-0.9278.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9278\n",
      "Epoch 100/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9247\n",
      "Epoch 00100: loss improved from 0.92777 to 0.92480, saving model to weights-friends_short-100-0.9248.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9248\n",
      "Epoch 101/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9185\n",
      "Epoch 00101: loss improved from 0.92480 to 0.91854, saving model to weights-friends_short-101-0.9185.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9185\n",
      "Epoch 102/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9169\n",
      "Epoch 00102: loss improved from 0.91854 to 0.91701, saving model to weights-friends_short-102-0.9170.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9170\n",
      "Epoch 103/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9097\n",
      "Epoch 00103: loss improved from 0.91701 to 0.91003, saving model to weights-friends_short-103-0.9100.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9100\n",
      "Epoch 104/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9062\n",
      "Epoch 00104: loss improved from 0.91003 to 0.90639, saving model to weights-friends_short-104-0.9064.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9064\n",
      "Epoch 105/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.9012\n",
      "Epoch 00105: loss improved from 0.90639 to 0.90138, saving model to weights-friends_short-105-0.9014.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.9014\n",
      "Epoch 106/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8980\n",
      "Epoch 00106: loss improved from 0.90138 to 0.89794, saving model to weights-friends_short-106-0.8979.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8979\n",
      "Epoch 107/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8937\n",
      "Epoch 00107: loss improved from 0.89794 to 0.89397, saving model to weights-friends_short-107-0.8940.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.8940\n",
      "Epoch 108/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8913\n",
      "Epoch 00108: loss improved from 0.89397 to 0.89142, saving model to weights-friends_short-108-0.8914.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.8914\n",
      "Epoch 109/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8882\n",
      "Epoch 00109: loss improved from 0.89142 to 0.88838, saving model to weights-friends_short-109-0.8884.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.8884\n",
      "Epoch 110/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8846\n",
      "Epoch 00110: loss improved from 0.88838 to 0.88459, saving model to weights-friends_short-110-0.8846.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8846\n",
      "Epoch 111/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8810\n",
      "Epoch 00111: loss improved from 0.88459 to 0.88074, saving model to weights-friends_short-111-0.8807.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8807\n",
      "Epoch 112/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8783\n",
      "Epoch 00112: loss improved from 0.88074 to 0.87859, saving model to weights-friends_short-112-0.8786.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8786\n",
      "Epoch 113/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8755\n",
      "Epoch 00113: loss improved from 0.87859 to 0.87555, saving model to weights-friends_short-113-0.8756.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8756\n",
      "Epoch 114/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8717\n",
      "Epoch 00114: loss improved from 0.87555 to 0.87178, saving model to weights-friends_short-114-0.8718.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8718\n",
      "Epoch 115/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8704\n",
      "Epoch 00115: loss improved from 0.87178 to 0.87049, saving model to weights-friends_short-115-0.8705.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8705\n",
      "Epoch 116/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8673\n",
      "Epoch 00116: loss improved from 0.87049 to 0.86742, saving model to weights-friends_short-116-0.8674.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8674\n",
      "Epoch 117/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8640\n",
      "Epoch 00117: loss improved from 0.86742 to 0.86429, saving model to weights-friends_short-117-0.8643.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8643\n",
      "Epoch 118/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8603\n",
      "Epoch 00118: loss improved from 0.86429 to 0.86053, saving model to weights-friends_short-118-0.8605.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8605\n",
      "Epoch 119/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8589\n",
      "Epoch 00119: loss improved from 0.86053 to 0.85884, saving model to weights-friends_short-119-0.8588.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8588\n",
      "Epoch 120/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8583\n",
      "Epoch 00120: loss improved from 0.85884 to 0.85831, saving model to weights-friends_short-120-0.8583.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8583\n",
      "Epoch 121/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8552\n",
      "Epoch 00121: loss improved from 0.85831 to 0.85535, saving model to weights-friends_short-121-0.8553.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8553\n",
      "Epoch 122/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8543\n",
      "Epoch 00122: loss improved from 0.85535 to 0.85436, saving model to weights-friends_short-122-0.8544.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8507\n",
      "Epoch 00123: loss improved from 0.85436 to 0.85059, saving model to weights-friends_short-123-0.8506.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8506\n",
      "Epoch 124/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8490\n",
      "Epoch 00124: loss improved from 0.85059 to 0.84910, saving model to weights-friends_short-124-0.8491.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8491\n",
      "Epoch 125/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8467\n",
      "Epoch 00125: loss improved from 0.84910 to 0.84677, saving model to weights-friends_short-125-0.8468.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8468\n",
      "Epoch 126/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8428\n",
      "Epoch 00126: loss improved from 0.84677 to 0.84257, saving model to weights-friends_short-126-0.8426.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8426\n",
      "Epoch 127/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8403\n",
      "Epoch 00127: loss improved from 0.84257 to 0.84043, saving model to weights-friends_short-127-0.8404.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8404\n",
      "Epoch 128/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8381\n",
      "Epoch 00128: loss improved from 0.84043 to 0.83788, saving model to weights-friends_short-128-0.8379.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8379\n",
      "Epoch 129/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8363\n",
      "Epoch 00129: loss improved from 0.83788 to 0.83660, saving model to weights-friends_short-129-0.8366.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.8366\n",
      "Epoch 130/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8348\n",
      "Epoch 00130: loss improved from 0.83660 to 0.83447, saving model to weights-friends_short-130-0.8345.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.8345\n",
      "Epoch 131/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8317\n",
      "Epoch 00131: loss improved from 0.83447 to 0.83192, saving model to weights-friends_short-131-0.8319.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.8319\n",
      "Epoch 132/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8283\n",
      "Epoch 00132: loss improved from 0.83192 to 0.82834, saving model to weights-friends_short-132-0.8283.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8283\n",
      "Epoch 133/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8262\n",
      "Epoch 00133: loss improved from 0.82834 to 0.82592, saving model to weights-friends_short-133-0.8259.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8259\n",
      "Epoch 134/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8258\n",
      "Epoch 00134: loss improved from 0.82592 to 0.82575, saving model to weights-friends_short-134-0.8258.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8258\n",
      "Epoch 135/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8243\n",
      "Epoch 00135: loss improved from 0.82575 to 0.82428, saving model to weights-friends_short-135-0.8243.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8243\n",
      "Epoch 136/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8211\n",
      "Epoch 00136: loss improved from 0.82428 to 0.82100, saving model to weights-friends_short-136-0.8210.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8210\n",
      "Epoch 137/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8211\n",
      "Epoch 00137: loss did not improve from 0.82100\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8211\n",
      "Epoch 138/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8184\n",
      "Epoch 00138: loss improved from 0.82100 to 0.81845, saving model to weights-friends_short-138-0.8184.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8184\n",
      "Epoch 139/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8153\n",
      "Epoch 00139: loss improved from 0.81845 to 0.81540, saving model to weights-friends_short-139-0.8154.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8154\n",
      "Epoch 140/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8129\n",
      "Epoch 00140: loss improved from 0.81540 to 0.81296, saving model to weights-friends_short-140-0.8130.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8130\n",
      "Epoch 141/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8135\n",
      "Epoch 00141: loss did not improve from 0.81296\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8136\n",
      "Epoch 142/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8112\n",
      "Epoch 00142: loss improved from 0.81296 to 0.81116, saving model to weights-friends_short-142-0.8112.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8112\n",
      "Epoch 143/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8114\n",
      "Epoch 00143: loss did not improve from 0.81116\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8114\n",
      "Epoch 144/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8092\n",
      "Epoch 00144: loss improved from 0.81116 to 0.80916, saving model to weights-friends_short-144-0.8092.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.8092\n",
      "Epoch 145/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8058\n",
      "Epoch 00145: loss improved from 0.80916 to 0.80551, saving model to weights-friends_short-145-0.8055.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.8055\n",
      "Epoch 146/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8069\n",
      "Epoch 00146: loss did not improve from 0.80551\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.8066\n",
      "Epoch 147/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8047\n",
      "Epoch 00147: loss improved from 0.80551 to 0.80470, saving model to weights-friends_short-147-0.8047.hdf5\n",
      "20867/20867 [==============================] - 49s 2ms/sample - loss: 0.8047\n",
      "Epoch 148/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8045\n",
      "Epoch 00148: loss improved from 0.80470 to 0.80430, saving model to weights-friends_short-148-0.8043.hdf5\n",
      "20867/20867 [==============================] - 56s 3ms/sample - loss: 0.8043\n",
      "Epoch 149/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8017\n",
      "Epoch 00149: loss improved from 0.80430 to 0.80183, saving model to weights-friends_short-149-0.8018.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.8018\n",
      "Epoch 150/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.8009\n",
      "Epoch 00150: loss improved from 0.80183 to 0.80065, saving model to weights-friends_short-150-0.8007.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.8007\n",
      "Epoch 151/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7980\n",
      "Epoch 00151: loss improved from 0.80065 to 0.79803, saving model to weights-friends_short-151-0.7980.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7980\n",
      "Epoch 152/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7967\n",
      "Epoch 00152: loss improved from 0.79803 to 0.79649, saving model to weights-friends_short-152-0.7965.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7965\n",
      "Epoch 153/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7956\n",
      "Epoch 00153: loss improved from 0.79649 to 0.79554, saving model to weights-friends_short-153-0.7955.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7955\n",
      "Epoch 154/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00154: loss improved from 0.79554 to 0.79283, saving model to weights-friends_short-154-0.7928.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7928\n",
      "Epoch 155/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7930\n",
      "Epoch 00155: loss did not improve from 0.79283\n",
      "20867/20867 [==============================] - 40s 2ms/sample - loss: 0.7929\n",
      "Epoch 156/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7905\n",
      "Epoch 00156: loss improved from 0.79283 to 0.79092, saving model to weights-friends_short-156-0.7909.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7909\n",
      "Epoch 157/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7887\n",
      "Epoch 00157: loss improved from 0.79092 to 0.78892, saving model to weights-friends_short-157-0.7889.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7889\n",
      "Epoch 158/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7882\n",
      "Epoch 00158: loss improved from 0.78892 to 0.78843, saving model to weights-friends_short-158-0.7884.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7884\n",
      "Epoch 159/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7864\n",
      "Epoch 00159: loss improved from 0.78843 to 0.78638, saving model to weights-friends_short-159-0.7864.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7864\n",
      "Epoch 160/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7844\n",
      "Epoch 00160: loss improved from 0.78638 to 0.78436, saving model to weights-friends_short-160-0.7844.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7844\n",
      "Epoch 161/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7847\n",
      "Epoch 00161: loss did not improve from 0.78436\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7846\n",
      "Epoch 162/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7826\n",
      "Epoch 00162: loss improved from 0.78436 to 0.78262, saving model to weights-friends_short-162-0.7826.hdf5\n",
      "20867/20867 [==============================] - 80s 4ms/sample - loss: 0.7826\n",
      "Epoch 163/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7812\n",
      "Epoch 00163: loss improved from 0.78262 to 0.78105, saving model to weights-friends_short-163-0.7810.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7810\n",
      "Epoch 164/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7800\n",
      "Epoch 00164: loss improved from 0.78105 to 0.77998, saving model to weights-friends_short-164-0.7800.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7800\n",
      "Epoch 165/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7790\n",
      "Epoch 00165: loss improved from 0.77998 to 0.77914, saving model to weights-friends_short-165-0.7791.hdf5\n",
      "20867/20867 [==============================] - 40s 2ms/sample - loss: 0.7791\n",
      "Epoch 166/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7780\n",
      "Epoch 00166: loss improved from 0.77914 to 0.77822, saving model to weights-friends_short-166-0.7782.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7782\n",
      "Epoch 167/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7758\n",
      "Epoch 00167: loss improved from 0.77822 to 0.77547, saving model to weights-friends_short-167-0.7755.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7755\n",
      "Epoch 168/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7774\n",
      "Epoch 00168: loss did not improve from 0.77547\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7774\n",
      "Epoch 169/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7757\n",
      "Epoch 00169: loss did not improve from 0.77547\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7756\n",
      "Epoch 170/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7735\n",
      "Epoch 00170: loss improved from 0.77547 to 0.77342, saving model to weights-friends_short-170-0.7734.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7734\n",
      "Epoch 171/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7715\n",
      "Epoch 00171: loss improved from 0.77342 to 0.77181, saving model to weights-friends_short-171-0.7718.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7718\n",
      "Epoch 172/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7717\n",
      "Epoch 00172: loss did not improve from 0.77181\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7721\n",
      "Epoch 173/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7693\n",
      "Epoch 00173: loss improved from 0.77181 to 0.76949, saving model to weights-friends_short-173-0.7695.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7695\n",
      "Epoch 174/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7675\n",
      "Epoch 00174: loss improved from 0.76949 to 0.76766, saving model to weights-friends_short-174-0.7677.hdf5\n",
      "20867/20867 [==============================] - 88s 4ms/sample - loss: 0.7677\n",
      "Epoch 175/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7667\n",
      "Epoch 00175: loss improved from 0.76766 to 0.76659, saving model to weights-friends_short-175-0.7666.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7666\n",
      "Epoch 176/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7668\n",
      "Epoch 00176: loss did not improve from 0.76659\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7667\n",
      "Epoch 177/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7654\n",
      "Epoch 00177: loss improved from 0.76659 to 0.76550, saving model to weights-friends_short-177-0.7655.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7655\n",
      "Epoch 178/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7670\n",
      "Epoch 00178: loss did not improve from 0.76550\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7669\n",
      "Epoch 179/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7647\n",
      "Epoch 00179: loss improved from 0.76550 to 0.76438, saving model to weights-friends_short-179-0.7644.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.7644\n",
      "Epoch 180/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7628\n",
      "Epoch 00180: loss improved from 0.76438 to 0.76294, saving model to weights-friends_short-180-0.7629.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7629\n",
      "Epoch 181/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7617\n",
      "Epoch 00181: loss improved from 0.76294 to 0.76154, saving model to weights-friends_short-181-0.7615.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7615\n",
      "Epoch 182/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7616\n",
      "Epoch 00182: loss improved from 0.76154 to 0.76150, saving model to weights-friends_short-182-0.7615.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7615\n",
      "Epoch 183/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7611\n",
      "Epoch 00183: loss improved from 0.76150 to 0.76127, saving model to weights-friends_short-183-0.7613.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7613\n",
      "Epoch 184/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7598\n",
      "Epoch 00184: loss improved from 0.76127 to 0.76031, saving model to weights-friends_short-184-0.7603.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7603\n",
      "Epoch 185/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7595\n",
      "Epoch 00185: loss improved from 0.76031 to 0.75945, saving model to weights-friends_short-185-0.7594.hdf5\n",
      "20867/20867 [==============================] - 44s 2ms/sample - loss: 0.7594\n",
      "Epoch 186/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00186: loss improved from 0.75945 to 0.75747, saving model to weights-friends_short-186-0.7575.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7575\n",
      "Epoch 187/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7575\n",
      "Epoch 00187: loss improved from 0.75747 to 0.75738, saving model to weights-friends_short-187-0.7574.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7574\n",
      "Epoch 188/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7562\n",
      "Epoch 00188: loss improved from 0.75738 to 0.75641, saving model to weights-friends_short-188-0.7564.hdf5\n",
      "20867/20867 [==============================] - 40s 2ms/sample - loss: 0.7564\n",
      "Epoch 189/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7557\n",
      "Epoch 00189: loss improved from 0.75641 to 0.75563, saving model to weights-friends_short-189-0.7556.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7556\n",
      "Epoch 190/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7566\n",
      "Epoch 00190: loss did not improve from 0.75563\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7564\n",
      "Epoch 191/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7540\n",
      "Epoch 00191: loss improved from 0.75563 to 0.75432, saving model to weights-friends_short-191-0.7543.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7543\n",
      "Epoch 192/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7523\n",
      "Epoch 00192: loss improved from 0.75432 to 0.75212, saving model to weights-friends_short-192-0.7521.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7521\n",
      "Epoch 193/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7519\n",
      "Epoch 00193: loss improved from 0.75212 to 0.75195, saving model to weights-friends_short-193-0.7520.hdf5\n",
      "20867/20867 [==============================] - 112s 5ms/sample - loss: 0.7520\n",
      "Epoch 194/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7489\n",
      "Epoch 00194: loss improved from 0.75195 to 0.74915, saving model to weights-friends_short-194-0.7492.hdf5\n",
      "20867/20867 [==============================] - 101s 5ms/sample - loss: 0.7492\n",
      "Epoch 195/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7504\n",
      "Epoch 00195: loss did not improve from 0.74915\n",
      "20867/20867 [==============================] - 81s 4ms/sample - loss: 0.7500\n",
      "Epoch 196/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7489\n",
      "Epoch 00196: loss improved from 0.74915 to 0.74896, saving model to weights-friends_short-196-0.7490.hdf5\n",
      "20867/20867 [==============================] - 43s 2ms/sample - loss: 0.7490\n",
      "Epoch 197/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7470\n",
      "Epoch 00197: loss improved from 0.74896 to 0.74699, saving model to weights-friends_short-197-0.7470.hdf5\n",
      "20867/20867 [==============================] - 41s 2ms/sample - loss: 0.7470\n",
      "Epoch 198/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7470\n",
      "Epoch 00198: loss improved from 0.74699 to 0.74697, saving model to weights-friends_short-198-0.7470.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7470\n",
      "Epoch 199/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7456\n",
      "Epoch 00199: loss improved from 0.74697 to 0.74543, saving model to weights-friends_short-199-0.7454.hdf5\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7454\n",
      "Epoch 200/200\n",
      "20850/20867 [============================>.] - ETA: 0s - loss: 0.7459\n",
      "Epoch 00200: loss did not improve from 0.74543\n",
      "20867/20867 [==============================] - 42s 2ms/sample - loss: 0.7459\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=200, callbacks=callbacks_list ) \n",
    "model.save( 'model_friends_short_epochs200.h5' ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokens_list.append( word_dict[ word ] ) \n",
    "        except:\n",
    "            continue\n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_question_len , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : one more time\n",
      " really end\n",
      "Enter question : exactly\n",
      " ohh yeah i already have one end\n",
      "Enter question : i have two\n",
      " i don't know end\n",
      "Enter question : i know\n",
      " i am so sorry end\n",
      "Enter question : i am not\n",
      " you're on my it's a new so end\n",
      "Enter question : so what\n",
      " i don't get it end\n",
      "Enter question : same here\n",
      " oh yeah there are a no end\n",
      "Enter question : there are a lot of no\n",
      " no it's not end\n",
      "Enter question : but not isn't no\n",
      " hey what's wrong end\n",
      "Enter question : everything\n",
      " no end\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_answer_len:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
