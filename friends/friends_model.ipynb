{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print( tf.VERSION )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of question is 16\n",
      "Max length of answer is 19\n",
      "(24380, 16)\n",
      "(24380, 19)\n",
      "(24380, 19, 8871)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.load( 'encoder_input_data_friends.npy' )\n",
    "decoder_input_data = np.load( 'decoder_input_data_friends.npy' )\n",
    "decoder_target_data = np.load( 'decoder_target_data_friends.npy' )\n",
    "\n",
    "embedding_matrix = np.load('embedding_matrix_friends.npy' ) \n",
    "\n",
    "tokenizer = pickle.load( open('tokenizer_friends.pkl' , 'rb'))\n",
    "\n",
    "num_tokens = len( tokenizer.word_index )+1\n",
    "word_dict = tokenizer.word_index\n",
    "\n",
    "max_question_len = encoder_input_data.shape[1]\n",
    "max_answer_len = decoder_input_data.shape[1]\n",
    "\n",
    "print( 'Max length of question is {}'.format( max_question_len) )\n",
    "print( 'Max length of answer is {}'.format( max_answer_len) )\n",
    "\n",
    "print( encoder_input_data.shape )\n",
    "print( decoder_input_data.shape )\n",
    "print( decoder_target_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\septo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    1774200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    1774200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8871)   1783071     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,973,071\n",
      "Trainable params: 5,973,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True , weights=[embedding_matrix] ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True, weights=[embedding_matrix]) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-friends-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2528\n",
      "Epoch 00001: loss improved from 1.25536 to 1.25281, saving model to weights-friends-01-1.2528.hdf5\n",
      "24380/24380 [==============================] - 73s 3ms/sample - loss: 1.2528\n",
      "Epoch 2/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2464\n",
      "Epoch 00002: loss improved from 1.25281 to 1.24685, saving model to weights-friends-02-1.2469.hdf5\n",
      "24380/24380 [==============================] - 89s 4ms/sample - loss: 1.2469\n",
      "Epoch 3/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2415\n",
      "Epoch 00003: loss improved from 1.24685 to 1.24151, saving model to weights-friends-03-1.2415.hdf5\n",
      "24380/24380 [==============================] - 95s 4ms/sample - loss: 1.2415\n",
      "Epoch 4/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2349\n",
      "Epoch 00004: loss improved from 1.24151 to 1.23529, saving model to weights-friends-04-1.2353.hdf5\n",
      "24380/24380 [==============================] - 107s 4ms/sample - loss: 1.2353\n",
      "Epoch 5/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2336\n",
      "Epoch 00005: loss improved from 1.23529 to 1.23343, saving model to weights-friends-05-1.2334.hdf5\n",
      "24380/24380 [==============================] - 85s 3ms/sample - loss: 1.2334\n",
      "Epoch 6/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.229 - ETA: 0s - loss: 1.2300\n",
      "Epoch 00006: loss improved from 1.23343 to 1.22975, saving model to weights-friends-06-1.2298.hdf5\n",
      "24380/24380 [==============================] - 113s 5ms/sample - loss: 1.2298\n",
      "Epoch 7/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2272\n",
      "Epoch 00007: loss improved from 1.22975 to 1.22681, saving model to weights-friends-07-1.2268.hdf5\n",
      "24380/24380 [==============================] - 71s 3ms/sample - loss: 1.2268\n",
      "Epoch 8/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2207\n",
      "Epoch 00008: loss improved from 1.22681 to 1.22056, saving model to weights-friends-08-1.2206.hdf5\n",
      "24380/24380 [==============================] - 76s 3ms/sample - loss: 1.2206\n",
      "Epoch 9/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2159\n",
      "Epoch 00009: loss improved from 1.22056 to 1.21606, saving model to weights-friends-09-1.2161.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.2161\n",
      "Epoch 10/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2108\n",
      "Epoch 00010: loss improved from 1.21606 to 1.21068, saving model to weights-friends-10-1.2107.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.2107\n",
      "Epoch 11/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2074\n",
      "Epoch 00011: loss improved from 1.21068 to 1.20760, saving model to weights-friends-11-1.2076.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.2076\n",
      "Epoch 12/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.2036\n",
      "Epoch 00012: loss improved from 1.20760 to 1.20338, saving model to weights-friends-12-1.2034.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.2034\n",
      "Epoch 13/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1995\n",
      "Epoch 00013: loss improved from 1.20338 to 1.19984, saving model to weights-friends-13-1.1998.hdf5\n",
      "24380/24380 [==============================] - 76s 3ms/sample - loss: 1.1998\n",
      "Epoch 14/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1960\n",
      "Epoch 00014: loss improved from 1.19984 to 1.19576, saving model to weights-friends-14-1.1958.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1958\n",
      "Epoch 15/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1925\n",
      "Epoch 00015: loss improved from 1.19576 to 1.19256, saving model to weights-friends-15-1.1926.hdf5\n",
      "24380/24380 [==============================] - 74s 3ms/sample - loss: 1.1926\n",
      "Epoch 16/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1871\n",
      "Epoch 00016: loss improved from 1.19256 to 1.18718, saving model to weights-friends-16-1.1872.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1872\n",
      "Epoch 17/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1845\n",
      "Epoch 00017: loss improved from 1.18718 to 1.18432, saving model to weights-friends-17-1.1843.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1843\n",
      "Epoch 18/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1800\n",
      "Epoch 00018: loss improved from 1.18432 to 1.18010, saving model to weights-friends-18-1.1801.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.1801\n",
      "Epoch 19/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1736\n",
      "Epoch 00019: loss improved from 1.18010 to 1.17407, saving model to weights-friends-19-1.1741.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.1741\n",
      "Epoch 20/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1726\n",
      "Epoch 00020: loss improved from 1.17407 to 1.17247, saving model to weights-friends-20-1.1725.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.1725\n",
      "Epoch 21/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1701\n",
      "Epoch 00021: loss improved from 1.17247 to 1.16996, saving model to weights-friends-21-1.1700.hdf5\n",
      "24380/24380 [==============================] - 67s 3ms/sample - loss: 1.1700\n",
      "Epoch 22/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1686\n",
      "Epoch 00022: loss improved from 1.16996 to 1.16865, saving model to weights-friends-22-1.1687.hdf5\n",
      "24380/24380 [==============================] - 85s 3ms/sample - loss: 1.1687\n",
      "Epoch 23/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1653\n",
      "Epoch 00023: loss improved from 1.16865 to 1.16541, saving model to weights-friends-23-1.1654.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1654\n",
      "Epoch 24/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1627\n",
      "Epoch 00024: loss improved from 1.16541 to 1.16287, saving model to weights-friends-24-1.1629.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1629\n",
      "Epoch 25/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1620\n",
      "Epoch 00025: loss improved from 1.16287 to 1.16215, saving model to weights-friends-25-1.1621.hdf5\n",
      "24380/24380 [==============================] - 81s 3ms/sample - loss: 1.1621\n",
      "Epoch 26/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1580\n",
      "Epoch 00026: loss improved from 1.16215 to 1.15774, saving model to weights-friends-26-1.1577.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1577\n",
      "Epoch 27/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1532\n",
      "Epoch 00027: loss improved from 1.15774 to 1.15338, saving model to weights-friends-27-1.1534.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1534\n",
      "Epoch 28/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1525\n",
      "Epoch 00028: loss improved from 1.15338 to 1.15254, saving model to weights-friends-28-1.1525.hdf5\n",
      "24380/24380 [==============================] - 82s 3ms/sample - loss: 1.1525\n",
      "Epoch 29/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1501\n",
      "Epoch 00029: loss improved from 1.15254 to 1.14952, saving model to weights-friends-29-1.1495.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1495\n",
      "Epoch 30/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1457\n",
      "Epoch 00030: loss improved from 1.14952 to 1.14613, saving model to weights-friends-30-1.1461.hdf5\n",
      "24380/24380 [==============================] - 88s 4ms/sample - loss: 1.1461\n",
      "Epoch 31/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1453\n",
      "Epoch 00031: loss improved from 1.14613 to 1.14541, saving model to weights-friends-31-1.1454.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1454\n",
      "Epoch 32/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1429\n",
      "Epoch 00032: loss improved from 1.14541 to 1.14292, saving model to weights-friends-32-1.1429.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.1429\n",
      "Epoch 33/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1422\n",
      "Epoch 00033: loss improved from 1.14292 to 1.14156, saving model to weights-friends-33-1.1416.hdf5\n",
      "24380/24380 [==============================] - 73s 3ms/sample - loss: 1.1416\n",
      "Epoch 34/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1390\n",
      "Epoch 00034: loss improved from 1.14156 to 1.13883, saving model to weights-friends-34-1.1388.hdf5\n",
      "24380/24380 [==============================] - 89s 4ms/sample - loss: 1.1388\n",
      "Epoch 35/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1327\n",
      "Epoch 00035: loss improved from 1.13883 to 1.13330, saving model to weights-friends-35-1.1333.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1333\n",
      "Epoch 36/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1334\n",
      "Epoch 00036: loss did not improve from 1.13330\n",
      "24380/24380 [==============================] - 78s 3ms/sample - loss: 1.1338\n",
      "Epoch 37/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1304\n",
      "Epoch 00037: loss improved from 1.13330 to 1.13041, saving model to weights-friends-37-1.1304.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1304\n",
      "Epoch 38/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1270\n",
      "Epoch 00038: loss improved from 1.13041 to 1.12656, saving model to weights-friends-38-1.1266.hdf5\n",
      "24380/24380 [==============================] - 72s 3ms/sample - loss: 1.1266\n",
      "Epoch 39/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1245\n",
      "Epoch 00039: loss improved from 1.12656 to 1.12426, saving model to weights-friends-39-1.1243.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1243\n",
      "Epoch 40/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1222\n",
      "Epoch 00040: loss improved from 1.12426 to 1.12245, saving model to weights-friends-40-1.1224.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1224\n",
      "Epoch 41/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1204\n",
      "Epoch 00041: loss improved from 1.12245 to 1.12079, saving model to weights-friends-41-1.1208.hdf5\n",
      "24380/24380 [==============================] - 71s 3ms/sample - loss: 1.1208\n",
      "Epoch 42/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1184\n",
      "Epoch 00042: loss improved from 1.12079 to 1.11839, saving model to weights-friends-42-1.1184.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1184\n",
      "Epoch 43/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1167\n",
      "Epoch 00043: loss improved from 1.11839 to 1.11687, saving model to weights-friends-43-1.1169.hdf5\n",
      "24380/24380 [==============================] - 82s 3ms/sample - loss: 1.1169\n",
      "Epoch 44/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1139\n",
      "Epoch 00044: loss improved from 1.11687 to 1.11356, saving model to weights-friends-44-1.1136.hdf5\n",
      "24380/24380 [==============================] - 73s 3ms/sample - loss: 1.1136\n",
      "Epoch 45/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1135\n",
      "Epoch 00045: loss did not improve from 1.11356\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.1136\n",
      "Epoch 46/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1124\n",
      "Epoch 00046: loss improved from 1.11356 to 1.11282, saving model to weights-friends-46-1.1128.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1128\n",
      "Epoch 47/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1088\n",
      "Epoch 00047: loss improved from 1.11282 to 1.10891, saving model to weights-friends-47-1.1089.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.1089\n",
      "Epoch 48/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1077\n",
      "Epoch 00048: loss improved from 1.10891 to 1.10755, saving model to weights-friends-48-1.1076.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.1076\n",
      "Epoch 49/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1044\n",
      "Epoch 00049: loss improved from 1.10755 to 1.10454, saving model to weights-friends-49-1.1045.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.1045\n",
      "Epoch 50/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1047\n",
      "Epoch 00050: loss improved from 1.10454 to 1.10443, saving model to weights-friends-50-1.1044.hdf5\n",
      "24380/24380 [==============================] - 78s 3ms/sample - loss: 1.1044\n",
      "Epoch 51/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.1014\n",
      "Epoch 00051: loss improved from 1.10443 to 1.10166, saving model to weights-friends-51-1.1017.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.1017\n",
      "Epoch 52/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0997\n",
      "Epoch 00052: loss improved from 1.10166 to 1.10022, saving model to weights-friends-52-1.1002.hdf5\n",
      "24380/24380 [==============================] - 71s 3ms/sample - loss: 1.1002\n",
      "Epoch 53/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0990\n",
      "Epoch 00053: loss improved from 1.10022 to 1.09901, saving model to weights-friends-53-1.0990.hdf5\n",
      "24380/24380 [==============================] - 77s 3ms/sample - loss: 1.0990\n",
      "Epoch 54/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0969\n",
      "Epoch 00054: loss improved from 1.09901 to 1.09695, saving model to weights-friends-54-1.0970.hdf5\n",
      "24380/24380 [==============================] - 87s 4ms/sample - loss: 1.0970\n",
      "Epoch 55/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0948\n",
      "Epoch 00055: loss improved from 1.09695 to 1.09516, saving model to weights-friends-55-1.0952.hdf5\n",
      "24380/24380 [==============================] - 127s 5ms/sample - loss: 1.0952\n",
      "Epoch 56/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0928\n",
      "Epoch 00056: loss improved from 1.09516 to 1.09251, saving model to weights-friends-56-1.0925.hdf5\n",
      "24380/24380 [==============================] - 75s 3ms/sample - loss: 1.0925\n",
      "Epoch 57/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0910\n",
      "Epoch 00057: loss improved from 1.09251 to 1.09139, saving model to weights-friends-57-1.0914.hdf5\n",
      "24380/24380 [==============================] - 72s 3ms/sample - loss: 1.0914\n",
      "Epoch 58/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0892\n",
      "Epoch 00058: loss improved from 1.09139 to 1.08918, saving model to weights-friends-58-1.0892.hdf5\n",
      "24380/24380 [==============================] - 68s 3ms/sample - loss: 1.0892\n",
      "Epoch 59/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0903\n",
      "Epoch 00059: loss did not improve from 1.08918\n",
      "24380/24380 [==============================] - 87s 4ms/sample - loss: 1.0904\n",
      "Epoch 60/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0877\n",
      "Epoch 00060: loss improved from 1.08918 to 1.08747, saving model to weights-friends-60-1.0875.hdf5\n",
      "24380/24380 [==============================] - 127s 5ms/sample - loss: 1.0875\n",
      "Epoch 61/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0877\n",
      "Epoch 00061: loss improved from 1.08747 to 1.08715, saving model to weights-friends-61-1.0872.hdf5\n",
      "24380/24380 [==============================] - 74s 3ms/sample - loss: 1.0872\n",
      "Epoch 62/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0868\n",
      "Epoch 00062: loss improved from 1.08715 to 1.08654, saving model to weights-friends-62-1.0865.hdf5\n",
      "24380/24380 [==============================] - 107s 4ms/sample - loss: 1.0865\n",
      "Epoch 63/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0845\n",
      "Epoch 00063: loss improved from 1.08654 to 1.08461, saving model to weights-friends-63-1.0846.hdf5\n",
      "24380/24380 [==============================] - 122s 5ms/sample - loss: 1.0846\n",
      "Epoch 64/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0824\n",
      "Epoch 00064: loss improved from 1.08461 to 1.08259, saving model to weights-friends-64-1.0826.hdf5\n",
      "24380/24380 [==============================] - 74s 3ms/sample - loss: 1.0826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0814\n",
      "Epoch 00065: loss improved from 1.08259 to 1.08113, saving model to weights-friends-65-1.0811.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0811\n",
      "Epoch 66/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0791\n",
      "Epoch 00066: loss improved from 1.08113 to 1.07854, saving model to weights-friends-66-1.0785.hdf5\n",
      "24380/24380 [==============================] - 76s 3ms/sample - loss: 1.0785\n",
      "Epoch 67/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0768\n",
      "Epoch 00067: loss improved from 1.07854 to 1.07702, saving model to weights-friends-67-1.0770.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.0770\n",
      "Epoch 68/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0751\n",
      "Epoch 00068: loss improved from 1.07702 to 1.07505, saving model to weights-friends-68-1.0750.hdf5\n",
      "24380/24380 [==============================] - 78s 3ms/sample - loss: 1.0750\n",
      "Epoch 69/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0746\n",
      "Epoch 00069: loss improved from 1.07505 to 1.07444, saving model to weights-friends-69-1.0744.hdf5\n",
      "24380/24380 [==============================] - 67s 3ms/sample - loss: 1.0744\n",
      "Epoch 70/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0731\n",
      "Epoch 00070: loss improved from 1.07444 to 1.07300, saving model to weights-friends-70-1.0730.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0730\n",
      "Epoch 71/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0736\n",
      "Epoch 00071: loss did not improve from 1.07300\n",
      "24380/24380 [==============================] - 71s 3ms/sample - loss: 1.0736\n",
      "Epoch 72/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0707\n",
      "Epoch 00072: loss improved from 1.07300 to 1.07019, saving model to weights-friends-72-1.0702.hdf5\n",
      "24380/24380 [==============================] - 90s 4ms/sample - loss: 1.0702\n",
      "Epoch 73/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0702\n",
      "Epoch 00073: loss improved from 1.07019 to 1.06998, saving model to weights-friends-73-1.0700.hdf5\n",
      "24380/24380 [==============================] - 76s 3ms/sample - loss: 1.0700\n",
      "Epoch 74/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0666\n",
      "Epoch 00074: loss improved from 1.06998 to 1.06673, saving model to weights-friends-74-1.0667.hdf5\n",
      "24380/24380 [==============================] - 83s 3ms/sample - loss: 1.0667\n",
      "Epoch 75/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0649\n",
      "Epoch 00075: loss improved from 1.06673 to 1.06503, saving model to weights-friends-75-1.0650.hdf5\n",
      "24380/24380 [==============================] - 81s 3ms/sample - loss: 1.0650\n",
      "Epoch 76/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0661\n",
      "Epoch 00076: loss did not improve from 1.06503\n",
      "24380/24380 [==============================] - 73s 3ms/sample - loss: 1.0659\n",
      "Epoch 77/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0625\n",
      "Epoch 00077: loss improved from 1.06503 to 1.06240, saving model to weights-friends-77-1.0624.hdf5\n",
      "24380/24380 [==============================] - 124s 5ms/sample - loss: 1.0624\n",
      "Epoch 78/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0614\n",
      "Epoch 00078: loss improved from 1.06240 to 1.06180, saving model to weights-friends-78-1.0618.hdf5\n",
      "24380/24380 [==============================] - 124s 5ms/sample - loss: 1.0618\n",
      "Epoch 79/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0604\n",
      "Epoch 00079: loss improved from 1.06180 to 1.06053, saving model to weights-friends-79-1.0605.hdf5\n",
      "24380/24380 [==============================] - 72s 3ms/sample - loss: 1.0605\n",
      "Epoch 80/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0601\n",
      "Epoch 00080: loss improved from 1.06053 to 1.06026, saving model to weights-friends-80-1.0603.hdf5\n",
      "24380/24380 [==============================] - 69s 3ms/sample - loss: 1.0603\n",
      "Epoch 81/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0563\n",
      "Epoch 00081: loss improved from 1.06026 to 1.05609, saving model to weights-friends-81-1.0561.hdf5\n",
      "24380/24380 [==============================] - 77s 3ms/sample - loss: 1.0561\n",
      "Epoch 82/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0578\n",
      "Epoch 00082: loss did not improve from 1.05609\n",
      "24380/24380 [==============================] - 100s 4ms/sample - loss: 1.0580\n",
      "Epoch 83/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0553\n",
      "Epoch 00083: loss improved from 1.05609 to 1.05509, saving model to weights-friends-83-1.0551.hdf5\n",
      "24380/24380 [==============================] - 80s 3ms/sample - loss: 1.0551\n",
      "Epoch 84/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0541\n",
      "Epoch 00084: loss improved from 1.05509 to 1.05397, saving model to weights-friends-84-1.0540.hdf5\n",
      "24380/24380 [==============================] - 97s 4ms/sample - loss: 1.0540\n",
      "Epoch 85/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0506\n",
      "Epoch 00085: loss improved from 1.05397 to 1.05105, saving model to weights-friends-85-1.0511.hdf5\n",
      "24380/24380 [==============================] - 80s 3ms/sample - loss: 1.0511\n",
      "Epoch 86/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0515\n",
      "Epoch 00086: loss did not improve from 1.05105\n",
      "24380/24380 [==============================] - 75s 3ms/sample - loss: 1.0514\n",
      "Epoch 87/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0517\n",
      "Epoch 00087: loss did not improve from 1.05105\n",
      "24380/24380 [==============================] - 71s 3ms/sample - loss: 1.0515\n",
      "Epoch 88/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0498\n",
      "Epoch 00088: loss improved from 1.05105 to 1.04938, saving model to weights-friends-88-1.0494.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0494\n",
      "Epoch 89/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0478\n",
      "Epoch 00089: loss improved from 1.04938 to 1.04769, saving model to weights-friends-89-1.0477.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0477\n",
      "Epoch 90/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0464\n",
      "Epoch 00090: loss improved from 1.04769 to 1.04702, saving model to weights-friends-90-1.0470.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0470\n",
      "Epoch 91/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0454\n",
      "Epoch 00091: loss improved from 1.04702 to 1.04614, saving model to weights-friends-91-1.0461.hdf5\n",
      "24380/24380 [==============================] - 71s 3ms/sample - loss: 1.0461\n",
      "Epoch 92/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0441\n",
      "Epoch 00092: loss improved from 1.04614 to 1.04379, saving model to weights-friends-92-1.0438.hdf5\n",
      "24380/24380 [==============================] - 77s 3ms/sample - loss: 1.0438\n",
      "Epoch 93/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0413\n",
      "Epoch 00093: loss improved from 1.04379 to 1.04125, saving model to weights-friends-93-1.0412.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0412\n",
      "Epoch 94/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0396\n",
      "Epoch 00094: loss improved from 1.04125 to 1.03987, saving model to weights-friends-94-1.0399.hdf5\n",
      "24380/24380 [==============================] - 72s 3ms/sample - loss: 1.0399\n",
      "Epoch 95/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0424\n",
      "Epoch 00095: loss did not improve from 1.03987\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0423\n",
      "Epoch 96/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0393\n",
      "Epoch 00096: loss improved from 1.03987 to 1.03895, saving model to weights-friends-96-1.0390.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0390\n",
      "Epoch 97/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0386\n",
      "Epoch 00097: loss improved from 1.03895 to 1.03863, saving model to weights-friends-97-1.0386.hdf5\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0393\n",
      "Epoch 00098: loss did not improve from 1.03863\n",
      "24380/24380 [==============================] - 70s 3ms/sample - loss: 1.0395\n",
      "Epoch 99/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0368\n",
      "Epoch 00099: loss improved from 1.03863 to 1.03672, saving model to weights-friends-99-1.0367.hdf5\n",
      "24380/24380 [==============================] - 71s 3ms/sample - loss: 1.0367\n",
      "Epoch 100/100\n",
      "24350/24380 [============================>.] - ETA: 0s - loss: 1.0348\n",
      "Epoch 00100: loss improved from 1.03672 to 1.03482, saving model to weights-friends-100-1.0348.hdf5\n",
      "24380/24380 [==============================] - 74s 3ms/sample - loss: 1.0348\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=100, callbacks=callbacks_list ) \n",
    "model.save( 'model_friends_epochs100.h5' ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            tokens_list.append( word_dict[ word ] ) \n",
    "        except:\n",
    "            continue\n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_question_len , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : hi!\n",
      " i know i have to just here end\n",
      "Enter question : hi\n",
      " hi end\n",
      "Enter question :  \n",
      " i know i have to just here end\n",
      "Enter question : what's up\n",
      " this uh go on the that's a big deal end\n",
      "Enter question : up\n",
      " you will hey end\n",
      "Enter question : whats up\n",
      " you will hey end\n",
      "Enter question : what up\n",
      " like i got get a end\n",
      "Enter question : where have you been\n",
      " oh just had a baby end\n",
      "Enter question : you just had a baby ?\n",
      " honey i got a end\n",
      "Enter question : what you got\n",
      " wanna have a come on him end\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_answer_len:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
